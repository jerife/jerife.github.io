{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seq2seq_code.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPiFTo7rYdVTOYfS2Xyg0Fx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"lhmV2_sgMI-I","executionInfo":{"status":"ok","timestamp":1623131792876,"user_tz":-540,"elapsed":2751,"user":{"displayName":"박재우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXI3etzUNsHKseHtKKPAfY_N2c7kAIWyXOMk2v=s64","userId":"10125744057017928750"}}},"source":["import torch\n","import torch.nn as nn"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"540aYfFeG_L9","executionInfo":{"status":"ok","timestamp":1623131794383,"user_tz":-540,"elapsed":5,"user":{"displayName":"박재우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXI3etzUNsHKseHtKKPAfY_N2c7kAIWyXOMk2v=s64","userId":"10125744057017928750"}}},"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n","        super().__init__()\n","\n","        # 임베딩(embedding)은 원-핫 인코딩(one-hot encoding)을 특정 차원의 임베딩으로 매핑하는 레이어\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","\n","        # LSTM 레이어\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n","        \n","        # 드롭아웃(dropout)\n","        self.dropout = nn.Dropout(dropout_ratio)\n","\n","    # 인코더는 소스 문장을 입력으로 받아 문맥 벡터(context vector)를 반환        \n","    def forward(self, text):\n","        embedded = self.dropout(self.embedding(text))\n","\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","      \n","        # 문맥 벡터(context vector) 반환\n","        return hidden, cell"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZFx8YPaiM_N","executionInfo":{"status":"ok","timestamp":1623131795703,"user_tz":-540,"elapsed":5,"user":{"displayName":"박재우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXI3etzUNsHKseHtKKPAfY_N2c7kAIWyXOMk2v=s64","userId":"10125744057017928750"}}},"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n","        super().__init__()\n","\n","        # 임베딩(embedding)은 원-핫 인코딩(one-hot encoding) 말고 특정 차원의 임베딩으로 매핑하는 레이어\n","        self.embedding = nn.Embedding(output_dim, embed_dim)\n","\n","        # LSTM 레이어\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n","        \n","        # FC 레이어 (인코더와 구조적으로 다른 부분)\n","        self.output_dim = output_dim\n","        self.fc_out = nn.Linear(hidden_dim, output_dim)\n","        \n","        # 드롭아웃(dropout)\n","        self.dropout = nn.Dropout(dropout_ratio)\n","\n","    # 디코더는 현재까지 출력된 문장에 대한 정보를 입력으로 받아 타겟 문장을 반환     \n","    def forward(self, input, hidden, cell):\n","        input = input.unsqueeze(0)\n","        \n","        embedded = self.dropout(self.embedding(input))\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","\n","        prediction = self.fc_out(output.squeeze(0))\n","        \n","        return prediction, hidden, cell"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_10e_jNMBjk","executionInfo":{"status":"ok","timestamp":1623131796948,"user_tz":-540,"elapsed":3,"user":{"displayName":"박재우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXI3etzUNsHKseHtKKPAfY_N2c7kAIWyXOMk2v=s64","userId":"10125744057017928750"}}},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    # 학습할 때는 완전한 형태의 소스 문장, 타겟 문장, teacher_forcing_ratio를 넣기\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        # 먼저 인코더를 거쳐 문맥 벡터(context vector)를 추출\n","        hidden, cell = self.encoder(src)\n","\n","        # 디코더(decoder)의 최종 결과를 담을 텐서 객체 만들기\n","        trg_len = trg.shape[0] # 단어 개수\n","        batch_size = trg.shape[1] # 배치 크기\n","        trg_vocab_size = self.decoder.output_dim # 출력 차원\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        # 첫 번째 입력은 항상 <sos> 토큰\n","        input = trg[0, :]\n","\n","        # 타겟 단어의 개수만큼 반복하여 디코더에 포워딩(forwarding)\n","        for t in range(1, trg_len):\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","\n","            outputs[t] = output # FC를 거쳐서 나온 현재의 출력 단어 정보\n","            top1 = output.argmax(1) # 가장 확률이 높은 단어의 인덱스 추출\n","\n","            # teacher_forcing_ratio: 학습할 때 실제 목표 출력(ground-truth)을 사용하는 비율\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            input = trg[t] if teacher_force else top1 # 현재의 출력 결과를 다음 입력에서 넣기\n","        \n","        return outputs"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"wC6f6bI3kVpb"},"source":[""],"execution_count":null,"outputs":[]}]}