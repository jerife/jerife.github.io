<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  <title>Introduce to Transformer</title>

  
  <meta name="author" content="Jerife">
  

  <meta name="description" content="Transformer with Attention">

  

  

  <link rel="alternate" type="application/rss+xml" title="Jerife" href="http://localhost:4000/feed.xml">

  

  

  

  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="Jerife">
  <meta property="og:title" content="Introduce to Transformer">
  <meta property="og:description" content="Transformer with Attention">

  
  <meta property="og:image" content="http://localhost:4000/assets/img/avatar-icon.png">
  

  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="Jerife">
  <meta property="og:article:published_time" content="2021-11-17T00:00:00-05:00">
  <meta property="og:url" content="http://localhost:4000/2021-11-17-transformer/">
  <link rel="canonical" href="http://localhost:4000/2021-11-17-transformer/">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  <meta property="twitter:title" content="Introduce to Transformer">
  <meta property="twitter:description" content="Transformer with Attention">

  
  <meta name="twitter:image" content="http://localhost:4000/assets/img/avatar-icon.png">
  

  


  

  
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/log.ico/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/log.ico/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/log.ico/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/log.ico/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/log.ico/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/log.ico/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/log.ico/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/log.ico/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/log.ico/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/log.ico/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/log.ico/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/log.ico/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/log.ico/favicon-16x16.png">
  <link rel="manifest" href="/assets/log.ico/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/log.ico/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
</head>




<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="http://localhost:4000/">Jerife</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/tags">Records</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/aboutme">About Me</a>
          </li></ul>
  </div>

  

  
    <div class="avatar-container">
      <div class="avatar-img-border">
        <a href="http://localhost:4000/">
          <img alt="Navigation bar avatar" class="avatar-img" src="/assets/img/avatar-icon.png" />
        </a>
      </div>
    </div>
  

</nav>


  <!-- TODO this file has become a mess, refactor it -->







<header class="header-section ">

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Introduce to Transformer</h1>
          
            
              <h2 class="post-subheading">Transformer with Attention</h2>
            
          

          
            <span class="post-meta">Posted on November 17, 2021</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      
        
        
        

        <div id="header-gh-btns">
          
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=jerife&repo=jerife.github.io&type=star&count=true" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=jerife&type=follow&count=true" frameborder="0" scrolling="0" width="220px" height="20px"></iframe>
              
            
          
        </div>
      

      

      <article role="main" class="blog-post">
        <div align="center"><h1>Transformer 이해하기</h1></div>
<!--break-->

<hr />

<p><br /></p>
<h1 id="transformer란">Transformer란?</h1>
<h6 id="기존의-ai의-nlp분야는-rnn계열의-lstm-gru-seq2seq등의-모델이-사용되었으며-추가적으로-attention-mechanism을-추가한-모델을-사용해왔습니다--">기존의 AI의 NLP분야는 RNN계열의 Lstm, GRU, Seq2Seq등의 모델이 사용되었으며, 추가적으로 Attention Mechanism을 추가한 모델을 사용해왔습니다. <br /> <br /></h6>
<h6 id="하지만-rnn계열의-고질적인-단점은-">하지만 RNN계열의 고질적인 <strong>“단점”은,</strong><br /> <br /></h6>
<h5 id="1-sequence한-데이터를-처음부터-끝까지-순차적으로-처리하기에-오랜-시간이-소요된다">1. Sequence한 데이터를 처음부터 끝까지 순차적으로 처리하기에 오랜 시간이 소요된다.</h5>
<h5 id="2-데이터의-크기가-큰경우-backpropagation시-gradient-vanishing기울기-소실이-발생한다-">2. 데이터의 크기가 큰경우, Backpropagation시 Gradient Vanishing(기울기 소실)이 발생한다.<br /> <br /></h5>
<h6 id="위와-같은-단점을-가지고-있습니다-이런-문제를-해결하기위해-rnn이나-cnn을-사용하지-않고-sequence한-데이터를-처리할-수-있는-모델인-transformer가-개발되었습니다-">위와 같은 단점을 가지고 있습니다. 이런 문제를 해결하기위해 RNN이나 CNN을 사용하지 않고 Sequence한 데이터를 처리할 수 있는 모델인 Transformer가 개발되었습니다.<br /> <br /></h6>
<p><img width="336" alt="transformer_img_1" src="https://user-images.githubusercontent.com/68190553/142149187-130fa040-eec5-4c89-ac4b-51ab2e9a12f0.png" class="mx-auto d-block" /></p>
<h6 id="transformer는-encoder파트-decoder파트로-구분되며-언어를-번역해주는-모델이라-가정을하고-설명하겠습니다">Transformer는 Encoder파트 Decoder파트로 구분되며, 언어를 번역해주는 모델이라 가정을하고 설명하겠습니다.</h6>

<hr />

<h2 id="encoder">[Encoder]</h2>
<h6 id="rnn계열의-sequence한-데이터를-처리하는-모델인-seq2seq를-기억하실겁니다-transformer도-seq2seq와-마찬가지로-encoder파트-decoder파트로-구분되며-각각은-데이터를-부호화하고-복호화하는-과정을-일컫습니다-">RNN계열의 Sequence한 데이터를 처리하는 모델인, Seq2Seq를 기억하실겁니다. Transformer도 Seq2Seq와 마찬가지로 Encoder파트 Decoder파트로 구분되며, 각각은 데이터를 부호화하고 복호화하는 과정을 일컫습니다.<br /> <br /></h6>
<h6 id="하지만-다른점은-rnn을-사용하지-않고-positional-encoding을-이용해-데이터의-위치성을-파악하고-scaled-dot-product-attention을-이용해-각각의-데이터가-서로-어떤-연관성을-가지고-있는지-파악함으로써-rnn을-대체합니다-">하지만 다른점은 RNN을 사용하지 않고, “Positional Encoding”을 이용해 데이터의 위치성을 파악하고 “Scaled Dot-Product Attention”을 이용해 각각의 데이터가 서로 어떤 연관성을 가지고 있는지 파악함으로써 RNN을 대체합니다.<br /> <br /></h6>
<h6 id="또한-scaled-dot-product-attention는-어떻게-사용되냐에-따라-self-attention---encoder-decoder-attention으로-나뉩니다">또한 Scaled Dot-Product Attention는 어떻게 사용되냐에 따라. “Self Attention” / “ Encoder-Decoder Attention”으로 나뉩니다.</h6>
<ul>
  <li>
    <h6 id="self-attention--스스로-attention함으로써-자기자신과의-연관성을-확인하는-것">Self Attention : 스스로 Attention함으로써, 자기자신과의 연관성을 확인하는 것</h6>
  </li>
  <li>
    <h6 id="encoder-decoder-attention--encoder와-decoder끼리-attention함으로써-집중해야할-것을-확인하는-것">Encoder-Decoder Attention : Encoder와 Decoder끼리 Attention함으로써, 집중해야할 것을 확인하는 것</h6>
  </li>
</ul>

<h6 id="scaled-dot-product-attention설명시-이-부분을-잘-숙지하시면서-이해하시길-바랍니다">Scaled Dot-Product Attention설명시 이 부분을 잘 숙지하시면서 이해하시길 바랍니다.</h6>

<p><img width="150" alt="transformer_img_2" src="https://user-images.githubusercontent.com/68190553/142150900-bc211eb6-e783-49de-b5c7-984b0c8efc28.png" class="mx-auto d-block" /></p>
<h5 id="실행-과정">[실행 과정]</h5>
<h6 id="1-우리는-번역하고자-하는-문장을-inputs에-넣은-후-모델이-인식할-수-있게-embedding해줍니다">1. 우리는 번역하고자 하는 문장을 inputs에 넣은 후, 모델이 인식할 수 있게 Embedding해줍니다.</h6>
<h6 id="2-embedding된-데이터문장의-위치적인-정보를-얻기-위해-positional-encoding-값을-구한-후-더해줍니다">2. Embedding된 데이터(문장)의 위치적인 정보를 얻기 위해 Positional Encoding 값을 구한 후 더해줍니다.</h6>
<h6 id="3-이후-multi-head-attention여러개의-scaled-dot-product-attention을-의미함를-진행합니다">3. 이후 Multi-Head Attention(여러개의 Scaled Dot-Product Attention을 의미함)를 진행합니다.</h6>
<h6 id="4-multi-head-attention을-진행한-값과-진행하기-이전-값을-residual-connection잔여학습-add해준-후-normalization정규화-norm까지-적용해줍니다">4. Multi-Head Attention을 진행한 값과 진행하기 이전 값을 Residual connection(잔여학습: Add)해준 후, Normalization(정규화: Norm)까지 적용해줍니다.</h6>
<h6 id="5-cnn모델에-등장하는-fully-connected를-진행해줍니다-출력을-필요한-형태로-변형함">5. CNN모델에 등장하는 Fully Connected를 진행해줍니다. (출력을 필요한 형태로 변형함)</h6>
<h6 id="6-fully-connected-layer를-지난-값과-이전-값을-residual-connection잔여학습-add해준-후-normalization정규화-norm까지-적용해줍니다--">6. Fully Connected Layer를 지난 값과 이전 값을 Residual connection(잔여학습: Add)해준 후, Normalization(정규화: Norm)까지 적용해줍니다. <br /> <br /></h6>

<h3 id="1-positional-encoding">[1] Positional Encoding</h3>
<p><img src="https://user-images.githubusercontent.com/68190553/142158745-4dc1d5e0-3bcf-4d57-a24c-63bb753d21df.png" alt="transformer_img_4" class="mx-auto d-block" /></p>
<h6 id="우리는-실행과정-1과-같이-inputs문장을-embedding해줍니다-논문에선-embedding의-dimension을-512로-해줬지만-위-이미지는-dimension은-4입니다-">우리는 (실행과정-1)과 같이 inputs(문장)을 Embedding해줍니다. 논문에선 Embedding의 Dimension을 512로 해줬지만, 위 이미지는 Dimension은 4입니다.<br /> <br /></h6>
<h6 id="우리는-rnn을-사용하지-않기-때문에-sequence적인시간의-흐름같은-특징을-알지-못합니다-그래서-단어의-위치를-의미해주는-값인-positional-encoding을-더해줍니다실행과정-2">우리는 RNN을 사용하지 않기 때문에, Sequence적인(시간의 흐름같은) 특징을 알지 못합니다. 그래서 단어의 위치를 의미해주는 값인 Positional Encoding을 더해줍니다.(실행과정-2)</h6>
<p><img src="https://user-images.githubusercontent.com/68190553/142161556-a9ec0cbd-7e9d-4fa4-ac2c-49e2214a65af.png" alt="transformer_img_6" class="mx-auto d-block" />
<img width="243" alt="transformer_img_5" src="https://user-images.githubusercontent.com/68190553/142160223-346657fb-f76b-4486-be5d-83e1279a7634.png" class="mx-auto d-block" /></p>
<h6 id="논문에선-sequence적인-위치를-표현하기-위해-sincos-함수를-이용해-값을-표현했습니다">논문에선 Sequence적인 위치를 표현하기 위해 sin/cos 함수를 이용해 값을 표현했습니다.</h6>
<h6 id="positional-encoding을-sincos-함수를-사용하는데-있어-장점은-">Positional Encoding을 sin/cos 함수를 사용하는데 있어 “장점”은,<br /> <br /></h6>
<h5 id="1-위-함수는--11사이의-값을-갖고있어-정규화되어-있다">1. 위 함수는 [-1,1]사이의 값을 갖고있어 정규화되어 있다.</h5>
<h5 id="2-연속적인-함수이기에-아무리-긴-input도-sequence적인-위치를-표현할수있다-">2. 연속적인 함수이기에 아무리 긴 input도 Sequence적인 위치를 표현할수있다.<br /> <br /></h5>
<h6 id="와-같은-장점이-있기에-논문에선-위와-같은-positional-encoding을-이용했습니다">와 같은 장점이 있기에, 논문에선 위와 같은 Positional Encoding을 이용했습니다.</h6>

<h3 id="2-multi-head-attention">[2] Multi-Head Attention</h3>
<p><img width="452" alt="transformer_img_7" src="https://user-images.githubusercontent.com/68190553/142338729-42bddaf4-1315-4330-b07c-c2189d217075.png" class="mx-auto d-block" /></p>
<h6 id="실행과정-3에-해당하는-multi-head-attention은-scaled-dot-product-attention-여러개를-병렬처리한-부분입니다-논문에선-8개의-self-attention을-한번에-처리했습니다">(실행과정-3)에 해당하는 Multi-Head Attention은 “Scaled Dot-Product Attention” 여러개를 병렬처리한 부분입니다. 논문에선 8개의 Self Attention을 한번에 처리했습니다.</h6>
<h6 id="그전에-scaled-dot-product-attention에-대해-먼저-알아보도록하죠-mask는-제외하고-진행하겠습니다">그전에 Scaled Dot-Product Attention에 대해 먼저 알아보도록하죠. (Mask는 제외하고 진행하겠습니다.)</h6>

<p><img src="https://user-images.githubusercontent.com/68190553/142338994-0a819f40-4fa4-430d-853e-d4c7563a9b40.png" alt="transformer_img_11" class="mx-auto d-block" /></p>
<h6 id="위사진-속엔-thinking-machines라는-문장을-embedding을-하고-positional-encoding이-더해진-초록색-2x4벡터가-있습니다-">위사진 속엔 “Thinking Machines”라는 문장을 Embedding을 하고, Positional Encoding이 더해진 초록색 2x4벡터가 있습니다.<br /> <br /></h6>
<h6 id="scaled-dot-product-attention에선-2x4벡터를-qquery-kkey-vvalue로-분해시킵니다-분해하는-과정에서-우리는-wq-wk-wv-를-dot-product해줌으로써-q-k-v를-얻습니다">Scaled Dot-Product Attention에선 2x4벡터를 Q(Query), K(Key), V(Value)로 분해시킵니다. 분해하는 과정에서 우리는 W^Q, W^K, W^v 를 Dot product해줌으로써 Q, K, V를 얻습니다.</h6>
<blockquote>
  <h6 id="q-k-v-각각의-단어가-서로-어떤-연관성을-보이는지-구할수-있게-해주는-값입니다">Q, K, V 각각의 단어가 서로 어떤 연관성을 보이는지 구할수 있게 해주는 값입니다.</h6>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/68190553/142339536-8ad3d7f3-870f-425e-9e1b-2fa17c5daabd.png" alt="transformer_img_12" class="mx-auto d-block" />
<img width="242" alt="transformer_img_16" src="https://user-images.githubusercontent.com/68190553/142353510-acbe492c-069f-4f91-9a2a-9423aa4c1609.png" class="mx-auto d-block" /></p>
<h6 id="그-후-위와-같은-연산을-해줌으로써-q의-각-단어가-k에-얼마나-영향이-있는지-확인하고dot-product-이를-정규화해준-후-확률값softmaxdim1을-취해줍니다-이-과정까진-2개의-스칼라-값을-가지게-됩니다-">그 후, 위와 같은 연산을 해줌으로써 Q의 각 단어가 K에 얼마나 영향이 있는지 확인하고(Dot product), 이를 정규화해준 후 확률값(softmax(dim=1))을 취해줍니다. (이 과정까진 2개의 스칼라 값을 가지게 됩니다.)<br /> <br /></h6>
<h6 id="이후-위에서-얻은-스칼라값과-v를-곱dim1해줍니다">이후 위에서 얻은 스칼라값과 V를 곱(dim=1)해줍니다.</h6>

<p><img src="https://user-images.githubusercontent.com/68190553/142340693-cbcdea3a-5a5e-4f7e-be88-355e21a95175.png" alt="transformer_img_15" class="mx-auto d-block" /></p>
<h6 id="위의-내용을-정리하면-이-사진과-같은-과정이-되는것이죠">위의 내용을 정리하면, 이 사진과 같은 과정이 되는것이죠.</h6>
<h6 id="밑에선부턴-이-과정을-동시에-여러번-동작되게-즉-여러게의-scaled-dot-product-attention을-병렬처리하는-multi-head-attention을-알아보도록하겠습니다">밑에선부턴 이 과정을 동시에 여러번 동작되게, 즉 여러게의 Scaled Dot-Product Attention을 병렬처리하는 Multi-Head Attention을 알아보도록하겠습니다.</h6>

<p><img src="https://user-images.githubusercontent.com/68190553/142340966-fd708d41-e200-45b5-b2bb-b4cede8d455b.png" alt="transformer_img_14" class="mx-auto d-block" /></p>
<h6 id="위-사진속에선-w0--w7까지-총-8개의-scaled-dot-product-attention을-병렬처리하는-내용을-보여줍니다-결국-z0z7의-갯수도-8개가-되는걸-알-수-있습니다-">위 사진속에선 W0 ~ W7까지 총 8개의 Scaled Dot-Product Attention을 병렬처리하는 내용을 보여줍니다. 결국 Z0~Z7의 갯수도 8개가 되는걸 알 수 있습니다.<br /> <br /></h6>
<h6 id="이제-이-z를-concatnationdim1해준-후-wo의-가중치와-합해줘서-최종-z맨-오른쪽의-값을-얻습니다-여기서-최종-z의-크기는-입력값으로-넣은-x2x4벡터와-같은-크기임을-알-수-있죠">이제 이 Z를 Concatnation(dim=1)해준 후, W^O의 가중치와 합해줘서 최종 Z(맨 오른쪽)의 값을 얻습니다. 여기서 최종 Z의 크기는 입력값으로 넣은 X(2x4벡터)와 “같은 크기”임을 알 수 있죠.</h6>

<h6 class="box-note" id="note-multi-head-attention혹은-에-값을-넣으면-입력값과-출력값의-크기가-같다"><strong>Note:</strong> Multi-Head Attention혹은 에 값을 넣으면, 입력값과 출력값의 크기가 같다.</h6>

<h6 id="입력값과-출력값의-크기가-같다는-것은-multi-head-attention을-직렬화-처리해서-여러번-붙여넣을-수도-있다는-의미입니다-실제로-논문에서도-encoder을-7번-직렬화시켰습니다-">입력값과 출력값의 크기가 같다는 것은 Multi-Head Attention을 직렬화 처리해서 여러번 붙여넣을 수도 있다는 의미입니다. 실제로 논문에서도 Encoder을 7번 직렬화시켰습니다.<br /> <br /></h6>

<h3 id="3-addnorm">[3] Add&amp;Norm</h3>
<h6 id="이제-실행과정-46에-대해-설명하겠습니다-이과정에선-multi-head-attentionfeed-forward를-거치기-이전-값과-이후-값을-서로-residual-connection더해줌를-진행해줍니다-">이제 (실행과정-4/6)에 대해 설명하겠습니다. 이과정에선, Multi-Head Attention/Feed Forward를 거치기 “이전 값”과 “이후 값”을 서로 Residual connection(더해줌)를 진행해줍니다.<br /> <br /></h6>
<h6 id="이전-값과-이후-값은-서로-크기가-같아-residual-connection더해줌이-가능합니다-추가적으로-normalization정규화-norm을-해줍니다-">“이전 값”과 “이후 값”은 서로 크기가 같아 Residual connection(더해줌)이 가능합니다! 추가적으로 Normalization(정규화: Norm)을 해줍니다.<br /> <br /></h6>

<h3 id="4-feed-forward">[4] Feed Forward</h3>
<h6 id="이는-우리가-흔히-알고있는-fully-connected를-의미하며-feed-forward층을-늘려줌으로써-연산과정을-한번-추가합니다--">이는 우리가 흔히 알고있는 Fully Connected를 의미하며, Feed Forward층을 늘려줌으로써 연산과정을 한번 추가합니다. <br /> <br /></h6>

<h2 id="decoder">[Decoder]</h2>
<p><img width="150" alt="transformer_img_3" src="https://user-images.githubusercontent.com/68190553/142353216-e47949d9-0439-4bc4-bf5f-0de9f31ddf10.png" class="mx-auto d-block" /></p>
<h6 id="decoder는-위에서-다룬-encoder와-비슷한-구조를-갖지만-multi-head-attention을-두번-진행한다는-차이점을-위조로-설명해보겠습니다-">Decoder는 위에서 다룬 Encoder와 비슷한 구조를 갖지만, Multi-Head Attention을 두번 진행한다는 차이점을 위조로 설명해보겠습니다.<br /> <br /></h6>

<h3 id="1-masked-multi-head-attention">[1] Masked Multi-Head Attention</h3>
<p><img width="452" alt="transformer_img_7" src="https://user-images.githubusercontent.com/68190553/142353716-0614e087-80ee-4c27-a49f-3fddafda7bcc.png" class="mx-auto d-block" /></p>
<h6 id="먼저-encoder에서-언급한-multi-head-attention-구조를-다시-살펴봅시다-encoder서의-scaled-dot-product-attentionself-attention을-할때는-maskopt을-제외하고-진행했습니다">먼저 Encoder에서 언급한 Multi-Head Attention 구조를 다시 살펴봅시다. Encoder서의 Scaled Dot-Product Attention(Self Attention)을 할때는 Mask(opt.)을 제외하고 진행했습니다.</h6>
<h6 id="밑에서-maskopt에-관한-언급을-하겠습니다">밑에서 Mask(opt.)에 관한 언급을 하겠습니다.</h6>
<p><img src="https://user-images.githubusercontent.com/68190553/142354221-023c01bb-47d8-4e5f-bae1-ef535271feab.png" alt="transformer_img_17" class="mx-auto d-block" /></p>
<h6 id="mask는-디코더-과정에서-타깃-단어-이후의-단어를-보지-않고-예측하기-위해-마스킹해주는-작업입니다-">Mask는 디코더 과정에서 타깃 단어 이후의 단어를 보지 않고 예측하기 위해, 마스킹해주는 작업입니다.<br /> <br /></h6>
<h5 id="실행-과정-1">[실행 과정]</h5>
<h6 id="1-qquery와-kkey를-dot-product해주고-정규화한-이후에-mask를-적용해줍니다-mask가-필요한-부분에--️를-대입해준다">1. Q(Query)와 K(Key)를 Dot product해주고 정규화한 이후에, Mask를 적용해줍니다. Mask가 필요한 부분에 -♾️를 대입해준다.</h6>
<h6 id="2-그리고-softmax를-취해준다">2. 그리고 Softmax를 취해준다.</h6>
<blockquote>
  <h6 id="-️-들어간-곳은-이후-softmax를-취해질-경우-0의-값을-가집니다">-♾️ 들어간 곳은 이후 Softmax를 취해질 경우 0의 값을 가집니다.</h6>
</blockquote>

<h6 class="box-note" id="note-mask를-취해줌으로써-다음-단어를-미리-알지-못한다"><strong>Note:</strong> Mask를 취해줌으로써 다음 단어를 미리 알지 못한다.</h6>

<h3 id="2-encoder-decoder-multi-head-attention">[2] Encoder-Decoder Multi-Head Attention</h3>
<h6 id="decoder의-두번째-multi-head-attention-부분을-보면--1개-값은qquery-masked-multi-head-attention에서-가져오고--2개의-값kkey-vvalue은-encoder로부터-받는-것을-알-수-있습니다-">Decoder의 두번째 Multi-Head Attention 부분을 보면,  1개 값은(Q(Query) Masked Multi-Head Attention에서 가져오고,  2개의 값(K(Key), V(Value)은 Encoder로부터 받는 것을 알 수 있습니다.<br /> <br /></h6>
<h5 id="왜-qquery를-decoder에서-받아올까요"><strong>왜 Q(query)를 Decoder에서 받아올까요?</strong></h5>
<h6 id="qquery는-조건에-해당하기-때문입니다-decoder에서-나온-결과중-어떤것에-집중해야할지를-encoder를-통해-알게됩니다--">Q(query)는 조건에 해당하기 때문입니다. Decoder에서 나온 결과중 어떤것에 집중해야할지를 Encoder를 통해 알게됩니다. <br /> <br /></h6>
<h6 id="qquery는-이미--masked-multi-head-attention에서-masking-됐으므로-i번째-위치까지만-attention을-받습니다-">Q(query)는 이미  Masked Multi-Head Attention에서 masking 됐으므로, i번째 위치까지만 Attention을 받습니다.<br /> <br /></h6>

<h6 id="decoder의-이-두가지-attetion과정-이외에는-encoder와-같으므로-설명을-생략하겠습니다">Decoder의 이 두가지 Attetion과정 이외에는 Encoder와 같으므로 설명을 생략하겠습니다.</h6>

<hr />
<h2 id="performance">[Performance]</h2>
<p><img src="https://user-images.githubusercontent.com/68190553/142361266-2d27964f-b2ac-4479-a07a-72ba31ec2b3b.png" alt="transformer_img_18" class="mx-auto d-block" /></p>
<blockquote>
  <h6 id="특정-encoderdecoder-layer의-attention의-해석을-시각화할-수-있음">특정 Encoder/Decoder Layer의 Attention의 해석을 시각화할 수 있음</h6>
</blockquote>

<h6 id="transformer어텐션-메커니즘의-유용한-속성은-해석이-가능하며-각-단어가-서로에게-어떤-연관성을-주는지-시각화해-확인할-수-있습니다">Transformer(어텐션 메커니즘)의 유용한 속성은 해석이 가능하며, 각 단어가 서로에게 어떤 연관성을 주는지 시각화해 확인할 수 있습니다.</h6>
<h6 id="encoder-layer의-attention을-시각화-한-자료를-마지막으로-보여드리면서-transformer에-대한-설명을-마치겠습니다-">Encoder Layer의 Attention을 시각화 한 자료를 마지막으로 보여드리면서, Transformer에 대한 설명을 마치겠습니다.<br /> <br /></h6>

<blockquote>
  <h5 id="reference">Reference</h5>
  <ul>
    <li>
      <h6 id="httpsjalammargithubioillustrated-transformer">https://jalammar.github.io/illustrated-transformer/</h6>
    </li>
    <li>
      <h6 id="httpsarxivorgabs170603762">https://arxiv.org/abs/1706.03762</h6>
    </li>
  </ul>
</blockquote>

      </article>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/tags#Natural Language Processing">Natural Language Processing</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->





      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/2021-11-16-attention/" data-toggle="tooltip" data-placement="top" title="Introduce to Attention Mechanism">&larr; Previous Post</a>
        </li>
        
        
      </ul>
      
  <div class="disqus-comments">
  <div class="comments">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
	  var disqus_shortname = 'beautiful-jekyll';
	  /* ensure that pages with query string get the same discussion */
	  var url_parts = window.location.href.split("?");
	  var disqus_url = url_parts[0];
	  (function() {
		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	  })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</div>
  
  

  




    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:jerife@naver.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/jerife" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Jerife
        &nbsp;&bull;&nbsp;
      
      2021

      
        &nbsp;&bull;&nbsp;
        <span class="author-site">
          <a href="http://localhost:4000/">Jerife.github.io</a>
        </span>
      

      
      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  









</body>
</html>
