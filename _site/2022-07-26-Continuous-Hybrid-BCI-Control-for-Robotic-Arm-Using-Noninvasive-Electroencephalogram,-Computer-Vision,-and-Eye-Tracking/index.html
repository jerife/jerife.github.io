<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head></head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!--$f(x) = x^2$->
  

  

  

  <title>Review - Continuous Hybrid BCI Control for Robotic Arm Using Noninvasive Electroencephalogram, Computer Vision, and Eye Tracking</title>

  
  <meta name="author" content="Jerife">
  

  <meta name="description" content="Hybrid BCI Control for Robotic Arm Using EEG, CV, ET">

  

  

  
  <link rel="alternate" type="application/rss+xml" title="Jerife" href="http://localhost:4000/feed.xml">
  

  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-H3TE8W3M78"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-H3TE8W3M78');
</script>

  

  

  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="Jerife">
  <meta property="og:title" content="Review - Continuous Hybrid BCI Control for Robotic Arm Using Noninvasive Electroencephalogram, Computer Vision, and Eye Tracking">
  <meta property="og:description" content="Hybrid BCI Control for Robotic Arm Using EEG, CV, ET">

  
  <meta property="og:image" content="http://localhost:4000/assets/img/avatar-icon.png">
  

  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="Jerife">
  <meta property="og:article:published_time" content="2022-07-26T00:00:00+09:00">
  <meta property="og:url" content="http://localhost:4000/2022-07-26-Continuous-Hybrid-BCI-Control-for-Robotic-Arm-Using-Noninvasive-Electroencephalogram,-Computer-Vision,-and-Eye-Tracking/">
  <link rel="canonical" href="http://localhost:4000/2022-07-26-Continuous-Hybrid-BCI-Control-for-Robotic-Arm-Using-Noninvasive-Electroencephalogram,-Computer-Vision,-and-Eye-Tracking/">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  <meta property="twitter:title" content="Review - Continuous Hybrid BCI Control for Robotic Arm Using Noninvasive Electroencephalogram, Computer Vision, and Eye Tracking">
  <meta property="twitter:description" content="Hybrid BCI Control for Robotic Arm Using EEG, CV, ET">

  
  <meta name="twitter:image" content="http://localhost:4000/assets/img/avatar-icon.png">
  

  


  

  
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/log.ico/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/log.ico/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/log.ico/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/log.ico/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/log.ico/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/log.ico/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/log.ico/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/log.ico/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/log.ico/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/log.ico/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/log.ico/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/log.ico/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/log.ico/favicon-16x16.png">
  <link rel="manifest" href="/assets/log.ico/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/log.ico/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
</head>

<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="http://localhost:4000/">Jerife</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/tags">Records</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/aboutme">About Me</a>
          </li>
        <li class="nav-item">
          <a class="nav-link" id="nav-search-link" href="#" title="Search">
            <span id="nav-search-icon" class="fa fa-search"></span>
            <span id="nav-search-text">Search</span>
          </a>
        </li></ul>
  </div>

  

  
    <div class="avatar-container">
      <div class="avatar-img-border">
        <a href="http://localhost:4000/">
          <img alt="Navigation bar avatar" class="avatar-img" src="/assets/img/avatar-icon.png" />
        </a>
      </div>
    </div>
  

</nav>



<div id="beautifuljekyll-search-overlay">

  <div id="nav-search-exit" title="Exit search">✕</div>
  <input type="text" id="nav-search-input" placeholder="Search">
  <ul id="search-results-container"></ul>
  
  <script src="https://unpkg.com/simple-jekyll-search@latest/dest/simple-jekyll-search.min.js"></script>
  <script>
    var searchjson = '[ \
       \
        { \
          "title"    : "Review - A multi-modal modified feedback self-paced BCI to control the gait of an avatar", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-08-17-A-multi-modal-modified-feedback-self-paced-BCI-to-control-the-gait-of-an-avatar/", \
          "date"     : "August 17, 2022" \
        }, \
       \
        { \
          "title"    : "Review - Continuous Hybrid BCI Control for Robotic Arm Using Noninvasive Electroencephalogram, Computer Vision, and Eye Tracking", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-07-26-Continuous-Hybrid-BCI-Control-for-Robotic-Arm-Using-Noninvasive-Electroencephalogram,-Computer-Vision,-and-Eye-Tracking/", \
          "date"     : "July 26, 2022" \
        }, \
       \
        { \
          "title"    : "Review - A Hybrid Speller Design Using Eye Tracking and SSVEP Brain–Computer Interface", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-07-13-A-Hybrid-Speller-Design-Using-Eye-Tracking-and-SSVEP-Brain-Computer-Interface/", \
          "date"     : "July 13, 2022" \
        }, \
       \
        { \
          "title"    : "Review - An EEG/EMG/EOG-Based Multimodal Human-Machine Interface to Real-Time Control of a Soft Robot Hand", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-07-04-An-EEG-EMG-EOG-Based-Multimodal-Human-Machine-Interface-to-Real-Time-Control-of-a-Soft-Robot-Hand/", \
          "date"     : "July  4, 2022" \
        }, \
       \
        { \
          "title"    : "Review - Motion Imagery-BCI Based on EEG and Eye Movement Data Fusion", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-07-03-Motion-Imagery-BCI-Based-on-EEG-and-Eye-Movement-Data-Fusion/", \
          "date"     : "July  3, 2022" \
        }, \
       \
        { \
          "title"    : "Review - Hybrid EEG-EOG-based BCI system for Vehicle Control", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-06-20-Hybrid-EEG-EOG-based-BCI-system-for-Vehicle-Control/", \
          "date"     : "June 20, 2022" \
        }, \
       \
        { \
          "title"    : "Review - An autonomous hybrid brain-computer interface system combined with eye-tracking in virtual environment", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-06-19-An-autonomous-hybrid-brain-computer-interface-system-combined-with-eye-tracking-in-virtual-environment/", \
          "date"     : "June 19, 2022" \
        }, \
       \
        { \
          "title"    : "Review - User-defined walking-in-place gestures for VR locomotion", \
          "category" : "Virtual Reality", \
          "url"      : "/2022-06-13-User-defined-walking-in-place-gestures-for-VR-locomotion/", \
          "date"     : "June 13, 2022" \
        }, \
       \
        { \
          "title"    : "Review - A Sliding Window Common Spatial Pattern for Enhancing Motor Imagery Classification in EEG-BCI", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-06-08-A-Sliding-Window-Common-Spatial-Pattern-for-Enhancing-Motor-Imagery-Classification-in-EEG-BCI/", \
          "date"     : "June  8, 2022" \
        }, \
       \
        { \
          "title"    : "Review - The Effect of Multisensory Pseudo-Haptic Feedback on Perception of Virtual Weight", \
          "category" : "Virtual Reality", \
          "url"      : "/2022-06-07-The-Effect-of-Multisensory-Pseudo-Haptic-Feedback-on-Perception-of-Virtual-Weight/", \
          "date"     : "June  7, 2022" \
        }, \
       \
        { \
          "title"    : "Review - A deep learning method for single-trial EEG classification in RSVP task based on spatiotemporal features of ERPs", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-06-02-A-deep-learning-method-for-single-trial-EEG-classification-in-RSVP-task-based-on-spatiotemporal-features-of-ERPs/", \
          "date"     : "June  2, 2022" \
        }, \
       \
        { \
          "title"    : "Review - Artificial Intelligence for the Metaverse - A Survey", \
          "category" : "Virtual Reality", \
          "url"      : "/2022-05-30-Artificial-Intelligence-for-the-Metaverse-A-Survey/", \
          "date"     : "May 30, 2022" \
        }, \
       \
        { \
          "title"    : "Review - A hybrid BCI-controlled smart home system combining SSVEP and EMG for individuals with paralysis", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-05-25-A-hybrid-BCI-controlled-smart-home-system-combining-SSVEP-and-EMG-for-individuals-with-paralysis/", \
          "date"     : "May 25, 2022" \
        }, \
       \
        { \
          "title"    : "Review - An integrated deep learning model for motor intention recognition of multi-class EEG Signals in upper limb amputees", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-05-24-An-integrated-deep-learning-model-for-motor-intention-recognition-of-multi-class-EEG-Signals-in-upper-limb-amputees/", \
          "date"     : "May 24, 2022" \
        }, \
       \
        { \
          "title"    : "Review - VR Locomotion in the New Era of Virtual Reality An Empirical Comparison of Prevalent Techniques", \
          "category" : "Virtual Reality", \
          "url"      : "/2022-05-23-VR-Locomotion-in-the-New-Era-of-Virtual-Reality-An-Empirical-Comparison-of-Prevalent-Techniques/", \
          "date"     : "May 23, 2022" \
        }, \
       \
        { \
          "title"    : "Review - Brain Controlled Wheelchairs A Robotic Architecture", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-05-18-Brain-Controlled-Wheelchairs-A-Robotic-Architecture/", \
          "date"     : "May 18, 2022" \
        }, \
       \
        { \
          "title"    : "Review - Robust Classification of EEG Signal for Brain Computer Interface", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-05-17-Robust-classification-of-EEG-signal-for-brain-computer-interface/", \
          "date"     : "May 17, 2022" \
        }, \
       \
        { \
          "title"    : "Introduce to LUKE", \
          "category" : "Natural Language Processing", \
          "url"      : "/2022-01-03-luke/", \
          "date"     : "January  3, 2022" \
        }, \
       \
        { \
          "title"    : "2021년 한 해를 돌아보는 회고록", \
          "category" : "Diary", \
          "url"      : "/2021-12-31-2021end/", \
          "date"     : "December 31, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to ELECTRA", \
          "category" : "Natural Language Processing", \
          "url"      : "/2021-12-11-electra/", \
          "date"     : "December 11, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to RoBERTa", \
          "category" : "Natural Language Processing", \
          "url"      : "/2021-12-07-roberta/", \
          "date"     : "December  7, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to BERT", \
          "category" : "Natural Language Processing", \
          "url"      : "/2021-11-19-bert/", \
          "date"     : "November 19, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to Transformer", \
          "category" : "Natural Language Processing", \
          "url"      : "/2021-11-17-transformer/", \
          "date"     : "November 17, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to Attention Mechanism", \
          "category" : "Natural Language Processing", \
          "url"      : "/2021-11-16-attention/", \
          "date"     : "November 16, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to EfficientDet", \
          "category" : "Computer VisionObject Detection", \
          "url"      : "/2021-11-15-efficientdet/", \
          "date"     : "November 15, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to Yolo v3", \
          "category" : "Computer VisionObject Detection", \
          "url"      : "/2021-10-29-yolov3/", \
          "date"     : "October 29, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to EfficientNet", \
          "category" : "Computer Vision", \
          "url"      : "/2021-10-28-efficientnet/", \
          "date"     : "October 28, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to Faster R-CNN", \
          "category" : "Computer VisionObject Detection", \
          "url"      : "/2021-09-12-fasterrcnn/", \
          "date"     : "September 12, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to Fast R-CNN", \
          "category" : "Computer VisionObject Detection", \
          "url"      : "/2021-09-10-fastrcnn/", \
          "date"     : "September 10, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to R-CNN", \
          "category" : "Computer VisionObject Detection", \
          "url"      : "/2021-07-25-rcnn/", \
          "date"     : "July 25, 2021" \
        }, \
       \
        { \
          "title"    : "Intriduce to ARIMA Model", \
          "category" : "Time Series", \
          "url"      : "/2021-06-26-arima/", \
          "date"     : "June 26, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to Seq2Seq", \
          "category" : "Natural Language ProcessingTime Series", \
          "url"      : "/2021-06-08-seq2seq/", \
          "date"     : "June  8, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to LSTM", \
          "category" : "Natural Language ProcessingTime Series", \
          "url"      : "/2021-06-06-lstm/", \
          "date"     : "June  6, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to RNN", \
          "category" : "Natural Language ProcessingTime Series", \
          "url"      : "/2021-06-05-rnn/", \
          "date"     : "June  5, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to DenseNet", \
          "category" : "Computer Vision", \
          "url"      : "/2021-05-12-densenet/", \
          "date"     : "May 12, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to ResNet", \
          "category" : "Computer Vision", \
          "url"      : "/2021-05-11-resnet/", \
          "date"     : "May 11, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to CNN", \
          "category" : "Computer Vision", \
          "url"      : "/2021-05-10-cnn/", \
          "date"     : "May 10, 2021" \
        }, \
       \
        { \
          "title"    : "To start blog with Github", \
          "category" : "Diary", \
          "url"      : "/2021-04-27-first/", \
          "date"     : "April 27, 2021" \
        }, \
       \
       \
        { \
          "title"    : "About me", \
          "category" : "page", \
          "url"      : "/aboutme/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "All Records", \
          "category" : "page", \
          "url"      : "/tags/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/page2/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/page3/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/page4/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/page5/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/page6/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/page7/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/page8/", \
          "date"     : "January 1, 1970" \
        } \
       \
    ]';
    searchjson = JSON.parse(searchjson);

    var sjs = SimpleJekyllSearch({
      searchInput: document.getElementById('nav-search-input'),
      resultsContainer: document.getElementById('search-results-container'),
      json: searchjson
    });
  </script>
</div>





  <!-- TODO this file has become a mess, refactor it -->







<header class="header-section ">

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Review - Continuous Hybrid BCI Control for Robotic Arm Using Noninvasive Electroencephalogram, Computer Vision, and Eye Tracking</h1>
          
            
              <h2 class="post-subheading">Hybrid BCI Control for Robotic Arm Using EEG, CV, ET</h2>
            
          

          
            <span class="post-meta">Posted on July 26, 2022</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
</div>
</header>



     
<div class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      
        
        
        

        <div id="header-gh-btns">
          
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=jerife&repo=jerife.github.io&type=star&count=true" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=jerife&type=follow&count=true" frameborder="0" scrolling="0" width="220px" height="20px"></iframe>
              
            
          
        </div>
      

      

      <article role="main" class="blog-post">
        <div align="center"><h2> Continuous Hybrid BCI Control for Robotic Arm Using Noninvasive Electroencephalogram, Computer Vision, and Eye Tracking 논문 리뷰</h2></div>
<!--break-->

<hr />

<p><br /></p>

<h2 id="introduction">Introduction</h2>
<p>현재 다양한 BCI가 발전중이지만 “robotic arm”의 경우 자유도(DOF)가 높기 때문에 BCI를 사용하여 3D 공간에서 여러 물체를 연속적이고 효과적인 방법으로 파악하는 것은 여전히 어려운 문제입니다.</p>

<p>따라서 본 논문은 MI-based EEG, computer vision, gaze detection 기술을 융합한 robotic arm을 제안합니다.</p>
<blockquote>
  <p>MI-based EEG의 장점은 SSVEP, P300과 다르게 외부의 자극에 의존하지 않으며 행동과 관련한 직접적인 신호를 전달할 수 있다는 점입니다.</p>
</blockquote>

<h2 id="2-methods-and-experiments">2. Methods and Experiments</h2>
<h3 id="a-21-system-architecture">A. 2.1. System Architecture</h3>
<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180722084-51b848eb-ce33-4363-82f7-b28282d3471e.png" width="100%" />
</div>
<p>먼저 task의 큰 흐름을 짚고 각 기술들의 detail을 확인하겠습니다.</p>

<p>참가자는 왼손, 오른손, 양손, 휴식(상상X)을 상상하여 화면 속 virtual ball(2D plane)을 각각 좌,우,상,하로 제어합니다. virtual ball은 robotic arm의 움직임과 같기 때문에 robotic arm도 해당방향으로 움직입니다.</p>

<p>robotic arm은 움직이면서 카메라를 통해 target, obstacle의 위치를 실시간으로 모니터에 feedback합니다.</p>

<p>참가자가 MI를 통해 robotic arm을 움직여 카메라로 target을 인지할 경우(모니터에 target이 뜬 경우) gaze detection 기술로 원하는 target을 선택합니다.</p>

<p>robotic arm은 선택한 target을 자동으로 그립하여 task를 마무리됩니다.</p>

<blockquote>
  <p>robotic arm이 움직이는 과정에선 ROS(robot operating system)가 obstacle를 인식하여 알아서 피합니다.</p>
</blockquote>

<h4 id="bci-system">BCI System</h4>
<p>데이터는 Fs = 500Hz로, 왼쪽 귀밑 reference 전극 포함 20개의 전극이 수집되었습니다.</p>

<p>이 후 50Hz notch filter, common average reference (CAR) filter가 적용되었습니다.</p>

<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180727843-fc047d6a-0cd4-4db5-8554-91928fc7590c.png" width="50%" />
</div>
<blockquote>
  <p>$V_{i}^{CAR}$는 시간 t에서 기준과 i번째 전극 사이의 전위이고, n은 이 연구에 사용된 전극의 수입니다.</p>
</blockquote>

<p>각 사용자에 대한 mu frequency band의 진폭을 평가하기 위해 다음과 같이 autoregressive(AR) 모델을 구축했습니다.</p>

<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180729308-a955ca8f-9a55-4116-899b-1fadd30e8fac.png" width="35%" />
</div>
<blockquote>
  <p>$a_{i}$ : weight coefficient</p>
</blockquote>

<p>본 논문에선 각 참가자의 sensorimotor rhythm amplitude를 이용해 ball과 robotic arm를 제어합니다.</p>

<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180729897-decc157e-4966-4847-b6e6-c92b64571f84.png" width="50%" />
</div>
<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180729946-7299874b-7103-4ecd-a1f8-9995289ea9cd.png" width="52%" />
</div>

<p>$K_{V}$ 가 negative or positive일 경우 the virtual ball은  up or down으로 움직입니다.<br />
$K_{H}$ 가 negative or positive일 경우 the virtual ball은  left or righ로 움직입니다.</p>

<p>$K_{V}$와 $K_{H}$의 weight($w$), offset($d$)은 previous trials에 따라 least-mean-square (LMS) algorithm으로 조정되어집니다.</p>

<h4 id="object-identification-and-target-selection">Object Identification and Target Selection</h4>
<p>본 논문의 computer vision은 online task의 target selection을 위해 object identification, gaze tracking 기술이 사용됩니다.</p>

<p>target은 4가지의 다른 색상이 존재하며 background subtraction algorithm을 기반에 의해 탐지되어집니다.</p>

<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180771570-497a6a91-f533-47b0-9365-2d6d2d67bba9.png" width="80%" />
</div>
<p>먼저 물체의 밝기를 완화하기 위해 RGB를 HSV로 변환합니다. <br />
이 후 target 이미지의 색상과 임계값의 차이에 따라 식별되어집니다. <br />
마지막으로 background subtraction에 의해 block이 식별되고 테두리가 생겨 목표물로 표시됩니다.</p>

<p>이 때 사용자는 gaze-control로 마우스를 목표물로 움진인 후 2초간 유지할 경우 선택되어집니다.</p>

<h4 id="robotic-arm-control-and-obstacle-avoidance">Robotic Arm Control and Obstacle Avoidance</h4>
<p>본 논문은 단순히 EEG data로만 실험했을 경우 정확도도 낮고 정신적인 부담감이 크기에 obstacle을 탐지하고 robotic arm에 정보(point cloud)를 전달할 수 있는 카메라를 workspace에 설치했습니다.</p>

<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180775426-ba71463b-fd6b-41b5-a174-9c1306efa4cb.png" width="100%" />
</div>
<p>workspace 외부에의 point clouds(정보)는 passthrough filter로 삭제되었습니다. <br />
voxel grid filter로 계산량을 줄이기 위해 downsampling됐습니다. <br />
outlier를 제거하기 위해 RadiusOutlierRemoval filter가 사용됐습니다. <br />
마지막으로 위 데이터는 Octomap로 변환되어집니다. <br /></p>

<p>결론적으로, robotic arm은 motion planning 단계에서 장애물을 식별하고 피합니다.</p>

<h3 id="b-experimental-paradigm">B. Experimental Paradigm</h3>
<h4 id="mi-without-feedback">MI without Feedback</h4>
<p>mian 실험 전, 모든 피실험자실험자마다의 서로 다른 MI의 차이를 maximizing는 최적의 frequencies, spatial channels, action of MI data를 얻기 위한 실험이 진행됩니다.</p>

<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180776495-31cd6581-5a79-41a1-b779-4227f9a32f14.png" width="60%" />
</div>
<blockquote>
  <p>상 : 양 손(팔)을 움직이는 상상<br />
하: 휴식 (아무 상상 x) <br />
좌: 왼 손(팔)을 움직이는 상상<br />
우: 오른 손(팔)을 움직이는 상상</p>
</blockquote>

<p>해당 실험은 2 session을 포함하고 각 session은 9 run으로 구성됩니다. <br />
1~3 run은 사진속의 (A)를 상상,<br />
4~6 run은 (B)를 상상,<br />
7~9 run은 자신의 임의대로 상상하여 상/하/좌/우에 해당하는 데이터를 수집합니다.</p>

<h4 id="virtual-ball-movement-control-training">Virtual Ball Movement Control Training</h4>
<p>MI를 기반으로 robotic arm을 지속적으로 제어하는 능력을 향상시키기 위해 피실험자들에게 MI에 의한 virtual ball control 훈련을 실시했습니다.</p>

<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180784162-ffef222d-8c75-4560-99c2-a672210690dc.png" width="80%" />
</div>
<blockquote>
  <p>stage 1 : Vertical MI task<br />
stage 2 : Horizental MI task<br />
stage 3 : 2D-MI task</p>
</blockquote>

<p>Virtual Ball Movement Control Training은 Virtual Ball을 색칠된 블록으로 이동시키는 task이며 stage 1~3 순서로 진행되었습니다.</p>

<h4 id="online-robotic-arm-control">Online Robotic Arm Control</h4>
<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180786494-18dbd9b7-f813-4c42-b84a-8eb48054daea.png" width="80%" />
</div>
<p>모니터는 virtual ball movement area와 the USB camera display area로 나눠져있으며 실험 환경은 위 figure와 같습니다.</p>

<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180786684-5b7202fc-322a-4c43-8e68-549bfe3f625b.png" width="80%" />
</div>
<p>실험이 시작되면 피실험자는 3초간 휴식 및 준비합니다. 이후 Virtual ball이 나타나면 피실험자는 MI를 통해 제어합니다.<br />
Virtual ball이 움직일때마다 camera display도 움직이며 camera display에 target이 나타나면 피실험자는 eye gaze로 마우스를 제어하여 target쪽으로 이동시킵니다.<br />
마우스를 해당 target에 2초간 유지할 경우 선택되어 robotic arm이 자동적으로 그랩합니다.</p>

<p>세부적인 detail은 아래와 같습니다.</p>
<ul>
  <li>task A : one target without an obstacle</li>
  <li>task B : one target with an obstacle</li>
  <li>task C : four targets with an obstacle, can select only one asked target</li>
</ul>

<h2 id="results">Results</h2>
<h3 id="a-performance-of-mi-without-feedback">A. Performance of MI without Feedback</h3>
<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180789265-8ead8367-4bcf-40cc-8bc5-ebec47a5814e.png" width="100%" />
</div>
<blockquote>
  <p>피실험자마다 최대 차이를 보여주는 특성을 얻기 위해 진행된 실험</p>
</blockquote>

<h3 id="b-performance-of-virtual-ball-control-training">B. Performance of Virtual Ball Control Training</h3>
<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180789427-8985ed3b-632a-4f91-9434-fabe65b59b63.png" width="100%" />
</div>
<blockquote>
  <p>A : Vertical MI task result<br />
B : Horizental MI task result<br />
C : 2D-MI task result<br />
D : average completion time</p>
</blockquote>

<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180789954-a06df71e-1f0b-469a-9479-a946fe970664.png" width="100%" />
</div>
<blockquote>
  <p>Virtual Ball Control Training을 진행한 후 Online 실험에서 MI의 특성이 더 극대화된 차이를 발생시킴</p>
</blockquote>

<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180790252-a7728c57-3887-40ee-a223-a64769030260.png" width="80%" />
</div>
<blockquote>
  <p>특히 ERD/ERS의 특징을 확인할 수 있음</p>
</blockquote>

<h3 id="performance-of-online-experiments">Performance of Online Experiments</h3>
<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180790569-5eaf3888-b9fe-4d7f-9729-465a4a0463e2.png" width="100%" />
</div>
<blockquote>
  <p>Online Experiments의 성공률과 소요시간</p>
</blockquote>

<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/180791166-f47b7926-4e4e-49aa-936c-bfbc0b164187.png" width="70%" />
</div>
<blockquote>
  <p>각 trial의 time–frequency spectral의 변화 (mu bands에서 큰 ERD/ERS를 볼 수 있음)</p>
</blockquote>

<h2 id="discussion--conclusions">Discussion &amp; Conclusions</h2>
<p>본 실험은 사전에 피실험자마다의 최적의 parameter를 구하고 Virtual Ball Control Training을 통해 MI control 능력을 향상시켰습니다.</p>

<p>MI control로 대략적인 위치를 제어한 후 computer vision and eye-tracker기술로 정확하게 타겟팅함으로써 뇌의 부담을 덜고 정확도를 향상시켰습니다.</p>

<p>또한 ROS기술을 응용하여 장애물을 자동으로 피하고 target을 그립할수 있도록 구현되었습니다.</p>

<p>결과적으로 ERD/ERS의 특성을 명백하게 보여줬으며, 사용자가 robotic arm의 이동을 조절할 수 있었기에 MI task는 차후 BCI제어에 필수적인 요소가 될것이라 주장합니다.</p>

<h6 id="reference">Reference</h6>
<ul>
  <li>DOI: 10.3390/math10040618</li>
</ul>

      </article>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/tags#Brain Computer Interface">Brain Computer Interface</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->





      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/2022-07-13-A-Hybrid-Speller-Design-Using-Eye-Tracking-and-SSVEP-Brain-Computer-Interface/" data-toggle="tooltip" data-placement="top" title="Review - A Hybrid Speller Design Using Eye Tracking and SSVEP Brain–Computer Interface">&larr; Previous Post</a>
        </li>
        
        
        <li class="page-item next">
          <a class="page-link" href="/2022-08-17-A-multi-modal-modified-feedback-self-paced-BCI-to-control-the-gait-of-an-avatar/" data-toggle="tooltip" data-placement="top" title="Review - A multi-modal modified feedback self-paced BCI to control the gait of an avatar">Next Post &rarr;</a>
        </li>
        
      </ul>
      
  <div class="disqus-comments">
  <div class="comments">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
	  var disqus_shortname = 'beautiful-jekyll';
	  /* ensure that pages with query string get the same discussion */
	  var url_parts = window.location.href.split("?");
	  var disqus_url = url_parts[0];
	  (function() {
		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	  })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</div>
  
  

  


  



    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:jerife@naver.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/jerife" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Jerife
        &nbsp;&bull;&nbsp;
      
      2022

      
        &nbsp;&bull;&nbsp;
        <span class="author-site">
          <a href="http://localhost:4000/">Jerife.github.io</a>
        </span>
      

      
      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  









</body>
</html>
