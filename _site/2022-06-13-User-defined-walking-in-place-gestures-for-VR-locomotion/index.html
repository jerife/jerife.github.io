<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head></head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <!-- 여기 -->
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9158339207139624"
     crossorigin="anonymous"></script>
  
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!--$f(x) = x^2$->
  

  

  

  <title>Review - User-defined walking-in-place gestures for VR locomotion</title>

  
  <meta name="author" content="Jerife">
  

  <meta name="description" content="Walking-In-Place for VR locomotion">

  

  

  
  <link rel="alternate" type="application/rss+xml" title="Jerife" href="http://localhost:4000/feed.xml">
  

  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-H3TE8W3M78"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-H3TE8W3M78');
</script>

  

  

  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="Jerife">
  <meta property="og:title" content="Review - User-defined walking-in-place gestures for VR locomotion">
  <meta property="og:description" content="Walking-In-Place for VR locomotion">

  
  <meta property="og:image" content="http://localhost:4000/assets/img/avatar-icon.png">
  

  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="Jerife">
  <meta property="og:article:published_time" content="2022-06-13T00:00:00+09:00">
  <meta property="og:url" content="http://localhost:4000/2022-06-13-User-defined-walking-in-place-gestures-for-VR-locomotion/">
  <link rel="canonical" href="http://localhost:4000/2022-06-13-User-defined-walking-in-place-gestures-for-VR-locomotion/">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  <meta property="twitter:title" content="Review - User-defined walking-in-place gestures for VR locomotion">
  <meta property="twitter:description" content="Walking-In-Place for VR locomotion">

  
  <meta name="twitter:image" content="http://localhost:4000/assets/img/avatar-icon.png">
  

  


  

  
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/log.ico/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/log.ico/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/log.ico/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/log.ico/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/log.ico/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/log.ico/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/log.ico/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/log.ico/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/log.ico/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/log.ico/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/log.ico/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/log.ico/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/log.ico/favicon-16x16.png">
  <link rel="manifest" href="/assets/log.ico/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/log.ico/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
</head>

<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="http://localhost:4000/">Jerife</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/tags">Records</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/aboutme">About Me</a>
          </li>
        <li class="nav-item">
          <a class="nav-link" id="nav-search-link" href="#" title="Search">
            <span id="nav-search-icon" class="fa fa-search"></span>
            <span id="nav-search-text">Search</span>
          </a>
        </li></ul>
  </div>

  

  
    <div class="avatar-container">
      <div class="avatar-img-border">
        <a href="http://localhost:4000/">
          <img alt="Navigation bar avatar" class="avatar-img" src="/assets/img/avatar-icon.png" />
        </a>
      </div>
    </div>
  

</nav>



<div id="beautifuljekyll-search-overlay">

  <div id="nav-search-exit" title="Exit search">✕</div>
  <input type="text" id="nav-search-input" placeholder="Search">
  <ul id="search-results-container"></ul>
  
  <script src="https://unpkg.com/simple-jekyll-search@latest/dest/simple-jekyll-search.min.js"></script>
  <script>
    var searchjson = '[ \
       \
        { \
          "title"    : "Review - A multi-modal modified feedback self-paced BCI to control the gait of an avatar", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-08-17-A-multi-modal-modified-feedback-self-paced-BCI-to-control-the-gait-of-an-avatar/", \
          "date"     : "August 17, 2022" \
        }, \
       \
        { \
          "title"    : "Review - Continuous Hybrid BCI Control for Robotic Arm Using Noninvasive Electroencephalogram, Computer Vision, and Eye Tracking", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-07-26-Continuous-Hybrid-BCI-Control-for-Robotic-Arm-Using-Noninvasive-Electroencephalogram,-Computer-Vision,-and-Eye-Tracking/", \
          "date"     : "July 26, 2022" \
        }, \
       \
        { \
          "title"    : "Review - A Hybrid Speller Design Using Eye Tracking and SSVEP Brain–Computer Interface", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-07-13-A-Hybrid-Speller-Design-Using-Eye-Tracking-and-SSVEP-Brain-Computer-Interface/", \
          "date"     : "July 13, 2022" \
        }, \
       \
        { \
          "title"    : "Review - An EEG/EMG/EOG-Based Multimodal Human-Machine Interface to Real-Time Control of a Soft Robot Hand", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-07-04-An-EEG-EMG-EOG-Based-Multimodal-Human-Machine-Interface-to-Real-Time-Control-of-a-Soft-Robot-Hand/", \
          "date"     : "July  4, 2022" \
        }, \
       \
        { \
          "title"    : "Review - Motion Imagery-BCI Based on EEG and Eye Movement Data Fusion", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-07-03-Motion-Imagery-BCI-Based-on-EEG-and-Eye-Movement-Data-Fusion/", \
          "date"     : "July  3, 2022" \
        }, \
       \
        { \
          "title"    : "Review - Hybrid EEG-EOG-based BCI system for Vehicle Control", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-06-20-Hybrid-EEG-EOG-based-BCI-system-for-Vehicle-Control/", \
          "date"     : "June 20, 2022" \
        }, \
       \
        { \
          "title"    : "Review - An autonomous hybrid brain-computer interface system combined with eye-tracking in virtual environment", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-06-19-An-autonomous-hybrid-brain-computer-interface-system-combined-with-eye-tracking-in-virtual-environment/", \
          "date"     : "June 19, 2022" \
        }, \
       \
        { \
          "title"    : "Review - User-defined walking-in-place gestures for VR locomotion", \
          "category" : "Virtual Reality", \
          "url"      : "/2022-06-13-User-defined-walking-in-place-gestures-for-VR-locomotion/", \
          "date"     : "June 13, 2022" \
        }, \
       \
        { \
          "title"    : "Review - A Sliding Window Common Spatial Pattern for Enhancing Motor Imagery Classification in EEG-BCI", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-06-08-A-Sliding-Window-Common-Spatial-Pattern-for-Enhancing-Motor-Imagery-Classification-in-EEG-BCI/", \
          "date"     : "June  8, 2022" \
        }, \
       \
        { \
          "title"    : "Review - The Effect of Multisensory Pseudo-Haptic Feedback on Perception of Virtual Weight", \
          "category" : "Virtual Reality", \
          "url"      : "/2022-06-07-The-Effect-of-Multisensory-Pseudo-Haptic-Feedback-on-Perception-of-Virtual-Weight/", \
          "date"     : "June  7, 2022" \
        }, \
       \
        { \
          "title"    : "Review - A deep learning method for single-trial EEG classification in RSVP task based on spatiotemporal features of ERPs", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-06-02-A-deep-learning-method-for-single-trial-EEG-classification-in-RSVP-task-based-on-spatiotemporal-features-of-ERPs/", \
          "date"     : "June  2, 2022" \
        }, \
       \
        { \
          "title"    : "Review - Artificial Intelligence for the Metaverse - A Survey", \
          "category" : "Virtual Reality", \
          "url"      : "/2022-05-30-Artificial-Intelligence-for-the-Metaverse-A-Survey/", \
          "date"     : "May 30, 2022" \
        }, \
       \
        { \
          "title"    : "Review - A hybrid BCI-controlled smart home system combining SSVEP and EMG for individuals with paralysis", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-05-25-A-hybrid-BCI-controlled-smart-home-system-combining-SSVEP-and-EMG-for-individuals-with-paralysis/", \
          "date"     : "May 25, 2022" \
        }, \
       \
        { \
          "title"    : "Review - An integrated deep learning model for motor intention recognition of multi-class EEG Signals in upper limb amputees", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-05-24-An-integrated-deep-learning-model-for-motor-intention-recognition-of-multi-class-EEG-Signals-in-upper-limb-amputees/", \
          "date"     : "May 24, 2022" \
        }, \
       \
        { \
          "title"    : "Review - VR Locomotion in the New Era of Virtual Reality An Empirical Comparison of Prevalent Techniques", \
          "category" : "Virtual Reality", \
          "url"      : "/2022-05-23-VR-Locomotion-in-the-New-Era-of-Virtual-Reality-An-Empirical-Comparison-of-Prevalent-Techniques/", \
          "date"     : "May 23, 2022" \
        }, \
       \
        { \
          "title"    : "Review - Brain Controlled Wheelchairs A Robotic Architecture", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-05-18-Brain-Controlled-Wheelchairs-A-Robotic-Architecture/", \
          "date"     : "May 18, 2022" \
        }, \
       \
        { \
          "title"    : "Review - Robust Classification of EEG Signal for Brain Computer Interface", \
          "category" : "Brain Computer Interface", \
          "url"      : "/2022-05-17-Robust-classification-of-EEG-signal-for-brain-computer-interface/", \
          "date"     : "May 17, 2022" \
        }, \
       \
        { \
          "title"    : "Introduce to LUKE", \
          "category" : "Natural Language Processing", \
          "url"      : "/2022-01-03-luke/", \
          "date"     : "January  3, 2022" \
        }, \
       \
        { \
          "title"    : "2021년 한 해를 돌아보는 회고록", \
          "category" : "Diary", \
          "url"      : "/2021-12-31-2021end/", \
          "date"     : "December 31, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to ELECTRA", \
          "category" : "Natural Language Processing", \
          "url"      : "/2021-12-11-electra/", \
          "date"     : "December 11, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to RoBERTa", \
          "category" : "Natural Language Processing", \
          "url"      : "/2021-12-07-roberta/", \
          "date"     : "December  7, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to BERT", \
          "category" : "Natural Language Processing", \
          "url"      : "/2021-11-19-bert/", \
          "date"     : "November 19, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to Transformer", \
          "category" : "Natural Language Processing", \
          "url"      : "/2021-11-17-transformer/", \
          "date"     : "November 17, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to Attention Mechanism", \
          "category" : "Natural Language Processing", \
          "url"      : "/2021-11-16-attention/", \
          "date"     : "November 16, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to EfficientDet", \
          "category" : "Computer VisionObject Detection", \
          "url"      : "/2021-11-15-efficientdet/", \
          "date"     : "November 15, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to Yolo v3", \
          "category" : "Computer VisionObject Detection", \
          "url"      : "/2021-10-29-yolov3/", \
          "date"     : "October 29, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to EfficientNet", \
          "category" : "Computer Vision", \
          "url"      : "/2021-10-28-efficientnet/", \
          "date"     : "October 28, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to Faster R-CNN", \
          "category" : "Computer VisionObject Detection", \
          "url"      : "/2021-09-12-fasterrcnn/", \
          "date"     : "September 12, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to Fast R-CNN", \
          "category" : "Computer VisionObject Detection", \
          "url"      : "/2021-09-10-fastrcnn/", \
          "date"     : "September 10, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to R-CNN", \
          "category" : "Computer VisionObject Detection", \
          "url"      : "/2021-07-25-rcnn/", \
          "date"     : "July 25, 2021" \
        }, \
       \
        { \
          "title"    : "Intriduce to ARIMA Model", \
          "category" : "Time Series", \
          "url"      : "/2021-06-26-arima/", \
          "date"     : "June 26, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to Seq2Seq", \
          "category" : "Natural Language ProcessingTime Series", \
          "url"      : "/2021-06-08-seq2seq/", \
          "date"     : "June  8, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to LSTM", \
          "category" : "Natural Language ProcessingTime Series", \
          "url"      : "/2021-06-06-lstm/", \
          "date"     : "June  6, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to RNN", \
          "category" : "Natural Language ProcessingTime Series", \
          "url"      : "/2021-06-05-rnn/", \
          "date"     : "June  5, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to DenseNet", \
          "category" : "Computer Vision", \
          "url"      : "/2021-05-12-densenet/", \
          "date"     : "May 12, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to ResNet", \
          "category" : "Computer Vision", \
          "url"      : "/2021-05-11-resnet/", \
          "date"     : "May 11, 2021" \
        }, \
       \
        { \
          "title"    : "Introduce to CNN", \
          "category" : "Computer Vision", \
          "url"      : "/2021-05-10-cnn/", \
          "date"     : "May 10, 2021" \
        }, \
       \
        { \
          "title"    : "To start blog with Github", \
          "category" : "Diary", \
          "url"      : "/2021-04-27-first/", \
          "date"     : "April 27, 2021" \
        }, \
       \
       \
        { \
          "title"    : "About me", \
          "category" : "page", \
          "url"      : "/aboutme/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "All Records", \
          "category" : "page", \
          "url"      : "/tags/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/page2/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/page3/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/page4/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/page5/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/page6/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/page7/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Jerife Blog", \
          "category" : "page", \
          "url"      : "/page8/", \
          "date"     : "January 1, 1970" \
        } \
       \
    ]';
    searchjson = JSON.parse(searchjson);

    var sjs = SimpleJekyllSearch({
      searchInput: document.getElementById('nav-search-input'),
      resultsContainer: document.getElementById('search-results-container'),
      json: searchjson
    });
  </script>
</div>





  <!-- TODO this file has become a mess, refactor it -->







<header class="header-section ">

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Review - User-defined walking-in-place gestures for VR locomotion</h1>
          
            
              <h2 class="post-subheading">Walking-In-Place for VR locomotion</h2>
            
          

          
            <span class="post-meta">Posted on June 13, 2022</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
</div>
</header>



     
<div class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      
        
        
        

        <div id="header-gh-btns">
          
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=jerife&repo=jerife.github.io&type=star&count=true" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=jerife&type=follow&count=true" frameborder="0" scrolling="0" width="220px" height="20px"></iframe>
              
            
          
        </div>
      

      

      <article role="main" class="blog-post">
        <div align="center"><h2> User-defined walking-in-place gestures for VR locomotion 논문 리뷰</h2></div>
<!--break-->

<hr />

<p><br /></p>

<h2 id="abstract">ABSTRACT</h2>
<p>Locomotion은 가장 기본이되는 VR interaction이며, VR Locomotion기술 중 Walking-in-place (WIP)는 실제로 걷는것과 같은 장점이 있습니다.</p>

<p>하지만 대부분의 WIP gestures는 user가 아닌 개발자의 관점에서 개발되었으므로, 학습 및 암기에 대한 인지 부하가 높을 뿐만 아니라 VR에서 존재감이 악화되고 감각 충돌이 증가할 수 있습니다.</p>

<p>따라서 본 연구에선 users로부터 WIP gestures를 도출해내고 평가하는 것을 목표로 합니다.</p>

<h2 id="introduction">INTRODUCTION</h2>
<p>WIP는 실제 보행보다 선호도가 낮지만, 실제 보행과 동일한 물리적 공간이 아닌 가상 공간을 위한 작은 물리적 공간이면 충분합니다.</p>

<p>또한, 대부분의 경우 hand-free할뿐 아니라 실제 보행과 유사한 움직임으로 전정 및 고유 감각 신호를 제공하여 기존 조이스틱 인터페이스에 비해 더 나은 몰입감과 공간 지각을 제공합니다.</p>

<p>기존의 WIP는 아래와 같은것 들이 존재합니다.</p>
<ul>
  <li>Stepping-in-place (SIP): 제자리를 걸어서 control</li>
  <li>Wiping-in-place : 바닥을 쓸어서 control</li>
  <li>Tapping-in-place : 발끝을 대고 발뒤꿈지만 들어서 control</li>
  <li>Swing-in-place : 한발을 들고 몸을 기울여서 control</li>
  <li>Jogging-in-place : 조깅하듯이 control</li>
  <li>그밖에도 상체나 머리를 기울여서 control하거나 등의 방법이 존재</li>
</ul>

<p>하지만 이외의 다양한 방향을 포함하여 서 있는 동안의 WIP gestures를 유도한 연구는 없습니다.<br />
또한 user가 앞으로 걷기에 step같은 WIP gestures를 선호하는지 의심스러우며, 옆, 뒤 걷기에 자연스럽고 사용하기 쉬운 gestures는 아직 알려지지 않았습니다.</p>

<p>따라서 본 연구는 user의 관점에서 앞, 옆, 뒤로 걷는 전체 방향을 포괄하는 완전한 WIP gestures 세트를 생성하는 것을 목표로 합니다. user가 도출해낸 gestures를 수집, 그룹화 및 평가합니다.</p>

<h2 id="experiment-1-elicitation-of-wip-gestures-for-vr-locomotion">Experiment 1: Elicitation of WIP gestures for VR locomotion</h2>
<h3 id="21-method">2.1. Method</h3>
<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/172992731-815a4ab7-74b5-4dd0-959b-16556195935d.png" width="80%" />
</div>

<p>실험의 모든 commands는 얼굴이 항상 정면을 향하고 가상 환경에서 해당 방향으로 걷는 것을 나타냅니다.</p>

<p>실험은 2가지 순서로 진행됩니다.</p>
<ol>
  <li>user가 직접 WIP gestures를 도출하는 실험</li>
  <li>기존에 존재하는 WIP gestures 방식 3가지(Tapping-in-place / Jogging-in-place / Swing-in-place)를 실험하고 0~7점까지 점수 부여</li>
</ol>

<h4 id="procedure">Procedure</h4>
<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/173000926-62e9ebda-0e07-4a4b-875d-f58e397ac12a.png" width="100%" />
</div>
<blockquote>
  <p>Reference Video: https://vimeo.com/527773000</p>
</blockquote>

<p>먼저, 참가자들에게 VR 헤드셋을 가슴 앞에 대고 8방향으로 각각 3보씩 2회 걷게 하고, 두 번째 시도에서의 보행 속도는 VR 헤드셋의 위치 추적으로 측정합니다.</p>

<p>이후 참가자들은 VR 헤드셋을 착용하고 무작위 순서로 3초 동안 8가지 명령을 각각 도출합니다. 앞서 측정한 속도에 따라 보행 속도를 설정하여 WIP gestures 유도 시 자연스러운 느낌을 제공했습니다.</p>

<p>WIP gestures가 완료되면 센서 카메라로 gestures를 기록하고 디자인 이유 및 보행 속도를 높이는 방법에 대한 구두 설명도 함께 기록합니다.</p>

<p>마지막으로, 참가자들은 명령을 표시하는 동안 3-4회 걷기 주기 동안 세 가지 기존의 WIP gestures 각각을 수행합니다.<br />
각 gestures를 두 번 수행한 직후 참가자들은 3가지 주관적 평가 항목인 7point Likert 척도(1=매우 동의하지 않음, 7=매우 동의함)로 평가하도록 요청받았습니다.</p>

<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/173268477-06c3c502-83ce-48bd-9558-d744176b01d4.png" width="60%" />
</div>
<blockquote>
  <p>Wizard of Oz technique 란? 실제로 구현되는 것이 아닌 상황을 보고 pilot이 직접 작동시키는 방식</p>
</blockquote>

<p>VR에서의 움직임은 Wizard of Oz technique로 구현됩니다.</p>

<h4 id="data-analysis">Data analysis</h4>
<p>20명의 참가자가 8 commands를 수행해 총 160 WIP gestures 데이터를 수집했습니다.</p>

<p>유도된 gestures는 먼저 direction과 movement 두 가지 구성요소로 분해하였고, gestures의 움직임 특성과 영상에 기록된 참여자의 구두설명을 고려하여 그룹화합니다.<br />
그런 다음 각 명령에 대해 일치율(AR)을 계산하여 다양한 참가자로부터 도출된 gestures 간의 일치 정도를 평가했습니다.</p>

<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/172996771-5c60317f-9928-4cc0-9676-6cc6c7e7295f.png" width="50%" />
</div>
<blockquote>
  <p>$P$ : c에 대해 유도된 gestures의 총 수<br />
$P_{i}$ : P에서 동일한 gestures의 하위 집합 i의 수</p>
</blockquote>

<ul>
  <li>AR &lt; 0.1일 때 일치</li>
  <li>0.1 ≤ AR &lt; 0.3일 때 중간 일치</li>
  <li>0.3 ≤ AR &lt; 0.5일 때 높은 일치</li>
  <li>AR ≥ 0.5일 때 매우 높은 일치</li>
</ul>

<p>기존에 존재했던 gestures 3가지는 ANOVA와 post hoc Tukey tests을 통해 비교됐습니다.</p>

<h3 id="22-results">2.2 Results</h3>
<h4 id="grouping-results-and-gestures-characteristics">Grouping results and gestures characteristics</h4>
<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/173000201-1bc4eff1-7f81-44cf-992e-18bb04045bfb.png" width="100%" />
</div>
<p>Table 1의 표와 같이 direction은 8개의 group, movement는 14개의 group이 도출되었습니다.</p>

<p>direction 중엔 Turn body, Step one foot<br />
movement의 경우 SIP, Rock, Stay가 우세적이었습니다.</p>

<p>디자인 이유에 따르면 참가자의 50%는 실제 걷는 것과 유사한 느낌을 주기 위해 동작 특성을 가장 많이 모방하고, 35%는 특정 방향으로 움직이는 느낌을 더 중시한다고 설명했습니다. 나머지는 gestures를 수행할 때 편안함을 극대화하는 것을 고려했다고 합니다.</p>

<h4 id="comparison-between-existing-gestures">Comparison between existing gestures</h4>
<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/173000870-6fbc689d-4ed7-4e9c-b033-fa5b686dd8c4.png" width="80%" />
</div>
<p>Tapping-in-place gestures가 goodness of fit, ease of use, perceived fatigue 부분에서 outperform 했습니다.</p>

<h3 id="23-discussion">2.3 Discussion</h3>

<p>direction는 대부분의 경우(80%) 참가자가 몸을 돌리거나 해당 방향으로 한 발을 내디뎠습니다.</p>

<p>Turn body는 전후방향으로 방향을 나타내기 위해 한 발보다 많이 선택되었고, 옆으로(±45º, ±90º, ±135º) 방향에서는 반대였습니다.<br />
90º 방향에서 더 많은 참가자가 몸을 90º로 돌리는 것보다 먼저 가까운 발을 들어 다른 쪽 발보다 높게 만들어 방향을 표시하는 것을 선호했습니다.</p>
<blockquote>
  <p>사람의 목은 80º까지 움직이는게 가능하기 때문에 자연스럽게 몸을 돌리는 것은 불편을 초래함</p>
</blockquote>

<p>movement의 경우 흔히알려진  SIP gestures가 가장 우세했습니다.<br />
한 가지 흥미로운 점은 0º 이외의 방향에서 SIP gestures를 사용하는 경향이 있다는 것입니다.</p>
<blockquote>
  <p>참가자가 지식과 경험을 새로운 지식과 경험으로 다양한 gestures를 시도한다는 것</p>
</blockquote>

<p>모든 gestures를 조사하는 대신 평균 2명 이상의 참가자가 제안한 gestures가 선택되었으며, 독특하게 디자인된 인기 없는 gestures는 더 나은 학습성과 기억 가능성을 위해 제외되었습니다.</p>

<p>추가로 existing gestures에서 가장 효과적이었던 Tapping-in-place gestures가 함께 조사되어집니다.</p>

<p>따라서 실험 2에서 아래의 WIP가 조사되어질 예정입니다.</p>
<ul>
  <li>Turn body + SIP (TSIP)</li>
  <li>Step one foot + SIP (SSIP)</li>
  <li>Step one foot + Rock (Rock)</li>
  <li>Step one foot + Stay (Stay)</li>
  <li>Exist (TIP)</li>
</ul>

<h2 id="experiment-2-follow-up-evaluation-of-user-elicited-and-existing-gestures-sets">Experiment 2: Follow-up evaluation of user-elicited and existing gestures sets</h2>
<h3 id="31-method">3.1. Method</h3>
<h4 id="experimental-settings-and-procedure">Experimental settings and procedure</h4>
<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/173268794-9ba1f1eb-8507-4eb9-91f0-254a9892b0d7.png" width="100%" />
</div>
<blockquote>
  <p>Reference Video: https://vimeo.com/469634346</p>
</blockquote>

<p>실험 세팅을 실험 1의 환경과 같게 구축했으며, 이동 속도의 경우 실험 1의 참가자들의 평균 이동속도를 사용했습니다.</p>

<p>먼저 참가자에게 실험 1에서 도출된 5가지 WIP를 video와 설명을 통해 이해시킵니다. 이후 gestures의 평가과정은 실험 1과 같게 진행됩니다.</p>

<p>실험 2에서 추가된 점은 gestures를 수행하기 전 / 후의 위치를 측정해 분석한다는 점입니다.</p>

<h4 id="data-analysis-1">Data analysis</h4>
<p>주관적인 평가 항목의 등급과 gestures 및 방향의 positional drift를 수집하여 통계적 분석을 위해 정리했습니다.</p>

<p>outlier를 찾기위해 Grubbs’ outlier test로 검증하고 outlier가 존재할 경우 제외시켰습니다.<br />
Ryan-Joiner test를 통해 normality(정상상태)를 검증하고 정규분포를 따르지 않을 경우 ANOVA, Tukey post hoc grouping tests로 검증했습니다.</p>

<h3 id="32-results">3.2. Results</h3>
<h4 id="subjective-evaluation-and-preference">Subjective evaluation and preference</h4>
<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/173269695-442231ca-22ca-44e1-8281-6a8ed1a8f3ba.png" width="100%" />
</div>
<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/173270256-b593a2db-fda9-4c44-8baf-1038a95c5509.png" width="50%" />
</div>
<blockquote>
  <p>각 방향에서 가장 선호하는 gestures의 빈도</p>
</blockquote>

<h4 id="positional-drift">Positional drift</h4>
<div align="center">
    <img src="https://user-images.githubusercontent.com/68190553/173286803-83f5770d-a81f-40f2-9cba-77ac6573473e.png" width="100%" />
</div>

<p>모든 경우에 평균 위치 편차는 0.03m에서 0.10m 사이였습니다.</p>

<p>TSIP 및 SSIP의 Positional drift가 Rock과 Stay의 Positional drift보다 더 큰 패턴이 발견되었습니다.</p>

<h3 id="33-discussion">3.3. Discussion</h3>
<h4 id="forward-walking">forward walking</h4>
<p>walking (0◦)의 경우 TSIP이 subjective evaluation과 overall preference에서 좋은 성능을 보였습니다.</p>

<p>이전 연구에서 SIP는 Tapping-in-place에 비해 신체적 제약이 더 높았던 반면,<br />
본 연구에선 Exist(Tapping-in-place)가 TSIP에 비해 적합성이 낮습니다. <br />
또한 피로, 자연스러움에서 유의한 차이가 발견되지 않았으며, 이에 관해 아래와 같은 요소들이 원인임을 추정합니다.</p>
<ol>
  <li>기술 구현의 차이 (본 연구는 Wizard of Oz technique 적용)</li>
  <li>움직이는 횟수의 차이 (본 연구는 각 방향당 6~8번 움직임)</li>
</ol>

<p>따라서, 더 긴 보행에서 Tapping-in-place를 적절하게 구현하면 SIP에 비해 동등한 적합성과 낮은 피로를 유도할 수 있다고 주장할 수 있습니다.</p>

<h4 id="sideways-and-backward-walking">sideways and backward walking</h4>
<p>sideways, backward walking의 경우는 Rock과 Stay가 ease of use, perceived fatigue 부분에서 좋은 성능을 보였습니다.</p>

<p>Stay는 주관적인 평가들의 결과에 따라 가장 좋은 gestures로 여겨질 수 있으며 편안하고 쉽기때문에, intense shooting, adventure, role-playing, gameplay scenarios와 같은 domain에 적용될 수 있습니다.</p>

<p>반면 Rock은 강한 몰입감을 주는 SSIP와 편안함을 제공하는 Stay의 장점을 갖기에 가장 많은 참가자들이 선호했습니다.</p>

<p>특히 Stay보다 Rock, SSIP가 더 많이 선호된다는 결과로, 이는 실험 1의 결과에서 참가자의 85%가 실제 보행과 유사한 느낌을 얻는 것을 강조한 결과와 반면 15%가 gestures를 유도할 때 편안함에 초점을 둔 결과와 일치합니다.</p>

<p>따라서, 현실감과 생생함이 참가자들이 VR 이동을 위한 gestures를 선택하는 중요한 요소라고 결론지을 수 있습니다.</p>

<h4 id="design-implications">Design implications</h4>
<ul>
  <li>SSIP는 현실감을 극대화하거나 어느 정도의 운동효과를 일으키기에 가장 적합한 선택이 될 것입니다.</li>
  <li>사용자가 장기간 편안하게 가상 환경을 탐색해야 하는 상황에서는 Stay가 최선의 선택이 될 수 있습니다.</li>
  <li>일반적으로 Rock은 효율성과 자연스러움 사이의 균형을 유지하면서 최상의 절충안으로 작용할 수 있습니다.</li>
  <li>또한, 앞으로 걷기를 위한 gestures는 가장 좋은 결과를 보였던 SIP gestures로 교체하는 것이 좋지만 이는 혼란으르 초래할 수 있습니다.</li>
</ul>

<h2 id="limitations-and-future-work">Limitations and future work</h2>
<p>이 연구에는 몇 가지 제한 사항이 있습니다.</p>
<ol>
  <li>본 연구는 freehand gestures에 대한 문화적 bias가 존재할 수 있습니다. 따라서 한국인이 아닌 사람에게 일반화될지는 의문입니다.</li>
  <li>Wizard of Oz technique을 기반으로 gestures를 평가했습니다. 따라서 gestures를 식별하는 기술들에 문제가 있을 수 있습니다.</li>
</ol>

<p>이후의 연구를 통해 표준화된 테스트에서 보다 현실적이고 장기적인 사용 사례에 따라 제안된 gestures를 검증하여 존재 및 멀미를 포함한 VR 관련 측면을 조사할 수 있습니다.</p>

<h2 id="conclusion">Conclusion</h2>
<p>먼저 사용자로부터 8개의 보행 방향(0◦, ±45◦, ±90◦, ±135◦, 180◦)으로 VR 이동을 위한 WIP gestures를 유도했습니다.
grouping 분석을 기반으로 Turn body + SIP, Step one foot + SIP/Rock/Stay WIP gestures를 선택하고 평가했습니다.</p>

<p>SIP는 직진에 좋은 성능을 보였고, Step one foot + SIP/Rock/Stay의 경우 효율성과 자연스러움 사이의 tradeoff한 다른 보행 방향에 대해 유망했습니다.</p>

<p>이러한 연구 결과는 user로부터 유도된 WIP gestures를 응용하여 VR locomotion에 더 나은 UX와 더 나은 이동 옵션을 제공할 수 있음을 시사했습니다.</p>

<hr />

<h2 id="고찰">고찰</h2>
<p>BCI와 접목한다면 딥러닝과 함께 4방향+sigmoid or 8방향+softmax등을 이용해 Motor Intention 부분을 연구할 수 있을 것 같다.</p>
<blockquote>
  <p>4방향+sigmoid (eg. 상,좌일 확률이 각각 50% 이상일 경우 좌상단으로 이동)</p>
</blockquote>

<p>하지만 이 경우, 정면을 향해서만 이동하기 때문에 Motor Imagery(binary classification)를 이용해 얼굴 방향으로 몸을 회전시키는 연구도 고려해볼 수 있을 것 같다.</p>

<h6 id="reference">Reference</h6>
<ul>
  <li>DOI: 10.1016/j.ijhcs.2021.102648</li>
</ul>

      </article>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/tags#Virtual Reality">Virtual Reality</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->





      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/2022-06-08-A-Sliding-Window-Common-Spatial-Pattern-for-Enhancing-Motor-Imagery-Classification-in-EEG-BCI/" data-toggle="tooltip" data-placement="top" title="Review - A Sliding Window Common Spatial Pattern for Enhancing Motor Imagery Classification in EEG-BCI">&larr; Previous Post</a>
        </li>
        
        
        <li class="page-item next">
          <a class="page-link" href="/2022-06-19-An-autonomous-hybrid-brain-computer-interface-system-combined-with-eye-tracking-in-virtual-environment/" data-toggle="tooltip" data-placement="top" title="Review - An autonomous hybrid brain-computer interface system combined with eye-tracking in virtual environment">Next Post &rarr;</a>
        </li>
        
      </ul>
      
  <div class="disqus-comments">
  <div class="comments">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
	  var disqus_shortname = 'beautiful-jekyll';
	  /* ensure that pages with query string get the same discussion */
	  var url_parts = window.location.href.split("?");
	  var disqus_url = url_parts[0];
	  (function() {
		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	  })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</div>
  
  

  


  



    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:jerife@naver.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/jerife" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Jerife
        &nbsp;&bull;&nbsp;
      
      2022

      
        &nbsp;&bull;&nbsp;
        <span class="author-site">
          <a href="http://localhost:4000/">Jerife.github.io</a>
        </span>
      

      
      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  









</body>
</html>
