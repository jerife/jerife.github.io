---
layout: post 
title: Introduce to RoBERTa
subtitle: Robustly Optimized BERT Pretraining Approach 
gh-repo: jerife/jerife.github.io
gh-badge: [star, follow]
tags: [Natural Language Processing]
comments: true
excerpt_separator: <!--break-->
---
<div align=center><h2>RoBERTa 이해하기</h2></div>
<!--break-->

----

 <br/>

## RoBERTa란?
RoBERTa는 Robustly Optimized BERT Pretraining Approach의 약자로, 이전 포스팅 ["Introduce to BERT"](https://jerife.github.io/2021-11-19-bert/) 에서 다룬 BERT에 더 최적화 된 하이퍼파라미터와 NSP기법을 제거하는 등의 실험을 더한 모델입니다.

기존 BERT와 RoBERTa는 4가지 차이점이 있습니다. 
* Static vs Dynamic Masking
* FULL-SENTENCES without NSP
* Large mini-batches
* Larger byte-level BPE
<br/>
  
---

### 1) Static vs Dynamic Masking
기존 BERT모델에선 input시에 Static하게 데이터를 Tokenize한 후, 모델에 학습으로 입력됐었습니다. 즉 데이터에 마스킹하는 작업을 미리 진행한 후, 입력되 때문에 **마스킹 된 단어들만 학습한다는 단점이 있습니다.** 

RoBERTa에선 위와같은 Static Masking에 대해 지적했습니다. 먼저 Tokenize된 데이터를 입력하면, 데이터를 10개 복사하여 각각에 랜덤하게 마스킹한 후, 모델링했습니다.
> 만약 40 epoch동안 학습시키면, 데이터를 10개로 복사하기 때문에, 같은 데이터를 4번 학습함

RoBERTa는 위 내용을 에폭마다 데이터가 들어오면 동적으로 다양한 마스킹이 할당되게  **Dynamic Masking** 을 제안했습니다.

<img width="438" alt="roberta_img_1" src="https://user-images.githubusercontent.com/68190553/145348820-40914cc4-ead2-4c0e-aa2b-13f7392bf7fa.png">{: .mx-auto.d-block :}
> F1로 측정된 SQuAD / accuracy으로 측정된 MNLI-m과 SST-2 

위 결과를 보면 Static Masking보다 더욱 다양하게 학습할 수 있는 Dynamic Masking 방법론이 더 좋은 점수를 내는 것을 알 수 있습니다.


### 2) FULL-SENTENCES without NSP
기존 BERT모델은 MLM(Masked Language Model)과 NSP(Next Sentence Prediction)  2가지 방법론으로 학습을 했었습니다. <br/>
RoBERTa에선 NSP 방법은 필요하지 않다는 가정하에, BERT모델에 NSP과정 제거함으로써 실험결과를 보여줍니다.

<img width="731" alt="roberta_img_2" src="https://user-images.githubusercontent.com/68190553/145352790-8e4177d9-254c-4a92-8204-55fa64c4ee0c.png">{: .mx-auto.d-block :}
> F1로 측정된 SQuAD / accuracy으로 측정된 MNLI-m, SST-2와 MNLI-m 
 
NSP loss를 사용하는 해 Pretrain한 모델과 그렇지 않은 모델과의 차이를 확인할 수 있습니다.


### 3) Large mini-batches
추가적으로 학습엔 영향이 거의 없다고 생각하는 BATCHSIZE를 크게 증가시킴으로써, 성능이 올랐다는 결과를 보여줍니다.
<img width="437" alt="roberta_img_3" src="https://user-images.githubusercontent.com/68190553/145357789-4f501529-0012-430e-a0c3-8947ec530c51.png">{: .mx-auto.d-block :}

위 사진을 보면 사실 배치사이즈와 비례하진 않지만, BATCHSIZE=256 보다 BATCHSIZE=2K가 더 좋은 성능을 보인다는 것을 알수있습니다.


### 4) Larger byte-level BPE
기존 BERT모델은 **"character-level BPE vocabulary of 30K"** 를 통해 단어들을 tokenize했지만, <br/>
RoBERTa에선 **"larger byte-level BPE vocabulary containing 50K"** 로 단어를 tokenize했습니다.
더 큰 단위로 tokeize하기 때문에 BERT_base and BERT_large의 파라미터가 각각 15M and 20M만큼 증가했습니다.

---

RoBERTa는 위 **4가지 과정**으로 BERT를 개선해나가면서 SOTA모델에 달성했습니다. <br/>
이상으로 RoBERTa의 대한 소개를 마치겠습니다. <br/> <br/>

###### Reference
*  https://arxiv.org/abs/1907.11692
