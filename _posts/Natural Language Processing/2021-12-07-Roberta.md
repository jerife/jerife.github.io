---
layout: post 
title: Introduce to RoBERTa
subtitle: Bidirectional Encoder Representations from Transformers 
gh-repo: jerife/jerife.github.io
gh-badge: [star, follow]
tags: [Natural Language Processing]
comments: true
excerpt_separator: <!--break-->
---
<div align=center><h2>RoBERTa 이해하기</h2></div>
<!--break-->

----

 <br/>
![bert_img_1](https://user-images.githubusercontent.com/68190553/142567208-ad01bb1f-4183-49fd-b176-5e95272a047a.jpeg){: .mx-auto.d-block :}
## RoBERTa란?sd
BERT는 Bidirectional Encoder Representations from Transformers의 약자로, 이전 포스팅인 ["Introduce to Transformer"](https://jerife.github.io/2021-11-17-transformer/) 에서 다룬 Transformers의 Encoder부분을 양방향으로 정보를 공유할 수 있게한 언어모델(Language Model)입니다. <br/> <br/>
BERT는 Wikipedia Corpus와 같은 방대한 양의 텍스트 데이터로 사전훈련된 언어모델(Language Model)입니다. 때문에 이미 Pretrained 된 모델이며, 우리가 어떻게 사용할지에 따라 Fine-tuning해나가는 모델입니다.<br/> <br/>

### [Differences in pre-training model]
<img width="671" alt="bert_img_3" src="https://user-images.githubusercontent.com/68190553/142567337-1cf6dbbf-ce52-4a63-8d9a-0d3684f80253.png">{: .mx-auto.d-block :}
위 사진속 모델은 각각, BERT / GPT / ELMo의 Architecture 입니다. <br/> <br/>
GPT는 left-to-right 방식으로 Transformer의 Decoder부분을 이용하며, ELMo는 독립적으로 각각 LSTM을 이용해 left-to-right, right-to-left 방식으로 특징을 얻어냅니다. <br/> <br/>
![bert_img_5](https://user-images.githubusercontent.com/68190553/142571487-32a123b3-6604-48dd-8c14-0d793f32c584.png){: .mx-auto.d-block :}
반면, BERT는 Transformer의 Encoder부분을 이용하며, 양방향으로 공동으로 조절되는 특징을 가지고 있습니다. 때문에 다양한 방법으로 Fine-tuning해, 다양한 Task(MNLI/NER/AQuAD/..)에 적용할 수 있습니다.
> 어떤 종류의 Task를 수행할 수 있는지는 밑에서 언급하며, 먼저 BERT의 Pre-training 과정에 대해서 먼저 언급하겠습니다. 

---

### [BERT Pre-training]
BERT는 방대한 양의 텍스트 데이터로 사전훈련된 언어모델(Language Model)이라 언급한 바에 있습니다. 그 학습 방법으로는 "MLM", "NSP"이렇게 두가지가 있으며, 그 전에 BERT에 적용된 기술과, 구조적인 특징에 대해 먼저 설명하겠습니다.<br/> <br/>

#### [1] BERT Embedding
<img width="535" alt="bert_img_2" src="https://user-images.githubusercontent.com/68190553/142575165-354daaa7-5c19-40c8-b18e-a393a57c5503.png">{: .mx-auto.d-block :}
##### 1-1) WordPiece Tokenizer (input)
BERT는 학습함에 있어, 단어를 최대한 쪼개어 학습을 진행했습니다. 때문에 input값에 BERT가 학습하지 않았던 단어들은 밑과 같이 분리됩니다.
* input : "The bird is flying."
* WordPiece Tokenizer(input) : "[CLS]", "The", "bird", "is", "fly", "##ing", "[SEP]"

"[CLS]"와 "[SEP]"은 각각 시작함을 알리고, 문장이 끝났다고 모델에게 인식시켜주기 위한 토큰입니다. Bert에는 값이 들어갈때는 이러한 구분자를 생성합니다.
>  [PAD] - 0 <br/>
 [UNK] - 100 <br/>
 [CLS] - 101<br/>
 [SEP] - 102<br/>
 [MASK] - 103<br/>

##### 1-2) Position Embedding
이전 글에서 다룬 Transformer은 Sin/Cos함수를 이용해 위치적인 벡터를 표현했습니다. 하지만 BERT에서는 Position Embedding또한 학습시켜서 위치적인 벡터를 얻습니다. <br/> <br/>BERT는 Squence(단어 1개)의 최대 길이(갯수)는 512이며, 총 512개의 Position Embedding 벡터가 학습됩니다.

##### 1-3) Segment Embedding
BERT는 한 문장만 학습할 수 있는것이 아니라, 두문장도 학습이 가능합니다. 
> *  sentence 1 : "The bird is flying."
*  sentence 2 : "The dog is running."

sentence 1 과 sentence 2를 이어 붙여 학습하고자 한다면, 아래와 같이 변할 것입니다.
> *  final sentence : "[CLS]", "The", "bird", "is", "fly", "##ing", "[SEP]", "The", "dog", "is", "run", "##ing", "[SEP]"

때문에 sentence 1 과 sentence 2를 구분할 수 있는 Embedding도 input으로 넣어줍니다. 위 이미지에서 Segment Embedding는 초록색 박스들을 의미합니다.

##### 1-4) Attention Mask
위 사진속에는 명시되어있지 않지만, 코딩할 경우 Attention Mask가 추가적으로 필요합니다. Attention Mask는 Attention해야할 부분을 BERT모델에 전달해주는 Embedding입니다.<br/> <br/>
Attention해야할 부분은 우리가 쓰거나 입력에 넣는 문장들입니다. 하지만 BERT는 512개의 Sequence(단어 1개)를 받기때문에, 빈칸은 "[PAD]"로 채워집니다.
> *  input : "The bird is flying."
*  WordPiece Tokenizer(input) : "[CLS]", "The", "bird", "is", "fly", "##ing", "[SEP]", "[PAD]", "[PAD]", "[PAD]"....., "[PAD]"(512번째)

때문에 Attention Mask는 "[PAD]"은 0으로 나머지는 1로 채워줍니다.
> *  1 1 1 1 1 1 1 0 0 0 0 0.....0(512번째)

BERT는 (1-2)과정을 통해 모든 단어와의 관계있는 문맥 벡터를 얻고, (1-2,3,4)과정과 함께 입력으로 들어갑니다. 그렇다면, BERT는 어떻게 모든 단어와의 관계있는 문맥을 반영한 임베딩을 얻게 되는 것일까요?

{: .box-warning} 
**BERT에는 Self Attention을 할 수 있는 layer가 있기 때문에 스스로의 연관성을 얻을 수 있습니다.**

 <br/>
#### [2] MLM(Masked Language Model)
 지금부터 BERT의 학습과정에 대해 본격적으로 이야기하며, 먼저 Label 없이도 스스로 학습할 수 있는 BERT의 특징에 대해서 이야기해보겠습니다. <br/> <br/>
 BERT는 스스로 데이터에 "[MASK]"를 주고, 그걸 맞춰나가면서 스스로 학습하는 언어모델(Language Model)입니다.
![bert_img_7](https://user-images.githubusercontent.com/68190553/142593647-4cb0539b-c040-4db7-a81a-bb77d6a78d67.png){: .mx-auto.d-block :}

> * "[MASK]"는 전체 데이터의 15%에만 적용한다.<br/>
* "[MASK]"가 적용된 15% 데이터 중에서, 80%는 "[MASK]"를 적용해서 맞춰나간다.<br/>
* "[MASK]"가 적용된 15% 데이터 중에서, 10%는 다른 단어로 변경해 맞춰나간다.<br/>
* "[MASK]"가 적용된 15% 데이터 중에서, 10%는 "[MASK]"나 다른 단어가 아니라 원래 단어로 유지해 맞춰나간다.

 "[MASK]"가 적용된 부분에 대해서만 예측하고, 그 부분만 학습을 합니다. 때문에 전체 데이터의 15%만 "[MASK]"를 적용하는 BERT의 특성상 전체 데이터의 15%만 학습할 수 있는 것입니다.<br/> <br/> <br/>

#### [3] NSP(Next Sentence Prediction)
<img width="351" alt="bert_img_8" src="https://user-images.githubusercontent.com/68190553/142637536-4aed8a53-28d3-4775-a07c-0af180abe36d.png">{: .mx-auto.d-block :}
 NSP는 말 그대로 "Next Sentence Prediction", 다음 문장을 예측하면서 학습해나가는 방법입니다. (두개의 문장을 주고, 두 문장이 이어지는 문장인지 아닌지를 맞추는 방법)

![bert_img_6](https://user-images.githubusercontent.com/68190553/142593554-33301fda-51d8-4ce1-bf0a-71076077aca6.png){: .mx-auto.d-block :}
 때문에 50:50 비율로 "한문장, 다음문장" / "한문장, 다른문장" 으로 input에 들어갑니다. BERT는 이 input을 가지고 "[CLS]" 토큰의 출력단에서 예측하고 학습합니다. <br/> <br/> <br/>

### [BERT Fine-tuning]
 BERT는 Pre-trainied되어진 언어모델(Language Model)라 했으며, 사용 용도에 따라 Fine-tuning을 진행할 수 있습니다.
<img width="574" alt="bert_img_4" src="https://user-images.githubusercontent.com/68190553/142638519-11efeb73-bea5-48c7-8fdd-1060adebd9a7.png">{: .mx-auto.d-block :}

> * [a] : 2개의 sentence의 관계를 가지고, 1개의 출력값을 얻을 수 있습니다.<br/>
* [b] : 1개의 sentence로 1개의 출력값을 얻으며, 대표적으로 감성분류가 있습니다.<br/>
* [c] : Q&A문제를 풀어낼수있습니다. 2개의 sentence로 각각 "질문", "답이 포함되어있는 문장"을 입력으로 넣으며, "답이 포함되어있는 문장" 속에 답에 해당하는 start index와 end index를 구합니다.<br/> 
* [d] : 1개의 sentence를 입력으로 넣어, 각각의 단어 성분에 대한 품사를 구합니다.

 위와 같이 BERT라는 언어모델(Language Model)은 용도에 따라 Fine-tuning 진행해 학습할 수 있습니다. BERT소개를 이상으로 마치겠습니다. <br/> <br/>


---
> ###### Reference
> *  https://arxiv.org/abs/1810.04805
> *  http://jalammar.github.io/illustrated-bert/
> *  https://wikidocs.net/115055
