---
layout: post 
title: Introduce to LUKE
subtitle: Deep Contextualized Entity Representations with Entity-aware Self-attention
gh-repo: jerife/jerife.github.io
gh-badge: [star, follow]
tags: [Natural Language Processing]
comments: true
use_math: true
excerpt_separator: <!--break-->
---
<div align=center><h2>LUKE 이해하기</h2></div>
<!--break-->

----


 <br/>

## LUKE란?
LUKE 또한 BERT 기반 모델인 ["Introduce to RoBERTa"](http://localhost:4000/2021-12-07-roberta/) 를 기반으로 한 모델입니다.<br/> 
"Language Understanding with Knowledge-based Embeddings" 라는 의미를 가지며, 문장에서 Word 뿐만 아니라 Entity 까지 사용해 학습하는 모델입니다.
> 논문에서 Entity란? Wikipedia에 하이퍼링크로 걸려있는 단어를 뜻합니다.

![img_luke_1](https://user-images.githubusercontent.com/68190553/147893652-8f0cdc81-c90d-4a5f-9745-e9b908d89333.png){: .mx-auto.d-block :}

#### LUKE 동작과정
1. “Beyonce lives in Los Angeles.” 문장이 LUKE에 input되고 각 Word와 Entity의 contextualized representation가 출력됩니다.
2. 모델은 랜덤하게 [MASK]된 Words('lives', 'Angeles')와 Entities('Los Angeles')를 예측합니다.
3. output representations를 linear classifiers 함으로써 적용하고자하는 Downstream tasks에 적용합니다.

---

### 1. Input Representation
#### Token embedding
LUKE에선 Word와 Entity를 구분하기에 Token embedding도 각각 두가지입니다. 

먼저 **Word**의 경우에는 Word 단어 임베딩을 따라가며, "**A**" 기호를 사용합니다.<br/>
반면 **Entity**의 경우에는 Entity 단어 임베딩을 따라가고, 계산의 효율성을 위해 Entity 단어 임베딩을 B와 U로 분해하여 "**BU**" 기호를 사용합니다.


#### Position embedding
각 단어의 위치를 나타내는 임베딩입니다. 

먼저 **Word**의 경우, "**$C_{i}$**" 기호를 사용합니다.<br/>
반면 **Entity**의 경우, "**$D_{i}$**"기호를 사용합니다.

하지만 Entity가 위 사진처럼 'Los' + 'Angless' 로 분리된 2가지 토큰이라면, 각 Position embedding 값을 평균해 사용합니다.

#### Entity type embedding 
Entity를 표현하는 단일 벡터 임베딩입니다.

<br/>
### 2. Entity-aware Self-attention
![img_luke_2](https://user-images.githubusercontent.com/68190553/147894991-2126af1a-2324-4ec3-8933-bded4f5457a8.png){: .mx-auto.d-block :}
> Self-attention 과정

기존 Transformer의 Self-attention과 다르게 entity-aware query mechanism를 사용하여 성능을 향상 시켰습니다.

![img_luke_3](https://user-images.githubusercontent.com/68190553/147895011-3e652707-4dbf-48c0-8d34-51a4760e6c28.png){: .mx-auto.d-block :}

 Entity-aware Self-attention에서 attention socre($e_{ij}$)는 downstream task에 따라 위 공식으로 구해집니다.

추가적으로 훈련 시간에 기울기를 계산하고 추가 쿼리 행렬 매개 변수를 업데이트하는 추가 과정을 제외하곤 원래 메커니즘과 제안된 메커니즘의 계산 비용동일하다고 합니다.

<br/>
### 3. Pretraining Task

![img_luke_5](https://user-images.githubusercontent.com/68190553/147895323-4fa121d9-7489-4c1c-9d12-dffff001e922.png){: .mx-auto.d-block :}

LUKE는 위키피디아의 하이퍼링크를 entity-annotations 으로 취급하고 위키피디아에서 검색된 대규모 entity-annotated 코퍼스를 사용하여 모델을 훈련합니다.

그리고 Entity의 15퍼센트를 [MASK]로 변환하고 이를 예측하면서 학습해나갑니다.
> 위 식에서 $h_{e}$가 [MASK] 변환까지 적용된 Entity입니다.


<br/>
### 4. Modeling Detail
나열해가면서 LUKE 모델의 Detail을 다뤄보겠습니다.

1. LUKE는 $RoBERTa_{LARGE}$ 를 기반으로한 모델이다.
2. Pretrain 과정에선 Word뿐만 아니라 Entity도 BERT의 MLM으로 학습합니다.
3. Entity 단어 임베딩에는 [MASK]와 [UNK] 두가지 특별한 토큰이 포함되어있다.
> [UNK] 토큰은 Entity로 선정된 단어가 'Entity 단어 임베딩' 에 없을 경우 대체됨 

4. RoBERTa의 파라미터의 수(355M) + entity embeddings의 파라미터의 수(128M) = 총 파라미터의 수는 483M 이다.
5. RoBERTa’s 토크나이저 의 수는 50K이며, 계산의 효율성을 위해 entity 단어 임베딩은 500K만 사용된다.

---
### Performance
![img_luke_4](https://user-images.githubusercontent.com/68190553/147895078-f1bcd485-15ab-4e04-a8d6-9ea78f3f364a.png){: .mx-auto.d-block :}

논문에선 기본 Self-attention을 사용했을 때보다 Entity-aware Self-attention 좋은 성능을 보임을 강조했습니다.

추가적으로 LUKE는 아래 5가지 task에서 SOTA를 달성하며, 성공적임 모델임을 증명합니다.
- entity typing on the Open Entity dataset (Choi et al., 2018)
- relation classification on the TACRED dataset (Zhang et al., 2017)
- NER on the CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003)
- clozestyle QA on the ReCoRD dataset (Zhang et al., 2018a)
- extractive QA on the SQuAD 1.1 dataset (Rajpurkar et al., 2016). 



###### Reference
*  https://arxiv.org/pdf/2010.01057.pdf