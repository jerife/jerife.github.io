---
layout: post 
title: Seq2Seq(Sequence to Sequence) 이해하기
subtitle: Seq2Seq의 Encoder / Decoder
gh-repo: jerife/jerife.github.io
gh-badge: [star, follow]
tags: [Natural Language Processing]
comments: true
---

## Seq2Seq이란?
Seq2Seq는 Sequence to Sequence의 약자로써, 시퀀스 데이터(언어, 시계열 데이터 등)를 또 다른 시퀀스 데이터로 변환해주는 모델입니다. <br/>
Seq2Seq 모델은 RNN모델(RNN, LSTM, GRU)등을 순환성을 기반으로 사용 되기에 [[RNN(Recurrent Neural Network) 이해하기](https://jerife.github.io/2021-06-05-rnn/)] | [[LSTM(Long Short-Term Memory) 이해하기](https://jerife.github.io/2021-06-06-lstm/)] 부분을 먼저 학습하셔야합니다. <br/>

![seq2seq_gif_1](https://user-images.githubusercontent.com/68190553/121118577-0ebd6280-c855-11eb-8717-5a1b232331ee.gif){: .mx-auto.d-block :} 
Seq2Seq 모델은 보이는 움짤과 같이 크게 **인코더 (Encoder)** 부분과 **디코더 (Decoder)** 부분으로 나눌 수 있습니다. <br/>
간단하게 인코더는 문자를 데이터로 변환시키고, 디코더는 데이터를 문자로 변환시키는 역할을 합니다. 

***
###  RNN of Seq2Seq
여기 Seq2Seq의 Encoder와 Decoder에선 RNN 기반 모델들을 (RNN, LSTM, GRU) 사용합니다. 이번글에선 LSTM 모델이 사용된다 가정하겠습니다. <br/>
![lstm_img_4](https://user-images.githubusercontent.com/68190553/121049358-97ef7d80-c7f2-11eb-8cf4-83117dffe5ba.png){: .mx-auto.d-block :} <br/>

LSTM모델은 시간에 따른 데이터를 순환 시켜주는 "ht" 값과 / 중요한 데이터는 기억하고, 아닌 데이터는 잊는 기억셀 "Ct"가 있습니다. 이 두 값은 다음셀로 계속 사용전달 되어, 다음셀에 영향을 줍니다. <br/>
이렇게 다음 층으로 계속 전달 되는 층을 **HIdden state** 라고도 부릅니다. <br/>

### Seq2Seq Architecture
Seq2Seq에서 사용 될 RNN모델을 정했기에 이제 Encoder / Decoder 와 이 둘을 이어주는 Context(문맥) 부분을 이해해보겠습니다. 
![image](https://user-images.githubusercontent.com/68190553/121048577-e2bcc580-c7f1-11eb-9c39-2cb0f717ce2d.gif){: .mx-auto.d-block :} 
#### [1] Encoder
<img width="434" alt="seq2seq_img_2" src="https://user-images.githubusercontent.com/68190553/121124975-c0ae5c00-c860-11eb-9e27-ed281412fcf8.png">{: .mx-auto.d-block :} 

Encoder부분을 보면 "je suis étudiant" 라는 문장이 Word level로(단어마다) RNN모델에 한개씩 들어가는 것을 알 수 있습니다. <br/>
셀에 단어가 들어갈떄 마다 **HIdden state**(LSTM의 기억셀 Ct와, 결과값 ht) 값이 나와서 다음셀에 영향을 주는 것을 볼 수 있습니다. <br/>
이 후 마지막 셀에 "étudiant" 단어가 들어가는 순간 **HIdden state**값이 나오고 그 값이 Decoder에 들어가는 것을 볼 수 있습니다. 우리는 Encoder부분의 마지막 셀에서 나오는 HIdden state를 **Context Vector**(문맥)이라고도 부릅니다.<br/>

#### [2] Context Vector
<img width="27" alt="seq2seq_img_4" src="https://user-images.githubusercontent.com/68190553/121125066-e2a7de80-c860-11eb-8fd7-20e3e2ebefb5.png">{: .mx-auto.d-block :}

**Context Vector**(문맥)으로 불리는 Encoder부분의 마지막 셀에서 나오는 HIdden state에 대해서 추가적인 설명을 하겠습니다. <br/>
Context Vector는 입력값("je suis étudiant")의 모든 정보를 모아 놓은 동시에, **고정된 크기의 벡터**이기도 합니다. <br/>
고정된 크기의 벡터는 많은양의 데이터도 꾹꾹 눌러서 압축시켜야하기 떄문에, 정보 손실률이 있다는 단점이 있지만, 차 후 **Attention Mechanism**을 통해 단점을 해결할 수 있게 됩니다. 이 글에선 Attention Mechanism에 대해선 다루지 않겠습니다.  <br/>

#### [3] Decoder
<img width="417" alt="seq2seq_img_3" src="https://user-images.githubusercontent.com/68190553/121125051-db80d080-c860-11eb-9b1c-038c2df283bc.png">{: .mx-auto.d-block :}

Decoder도 Encoder과 마찬가지로 정보를 받아, RNN을 통해 결과를 예측하는 것 까지는 같습니다.
Decoder는 유일한 정보인 Context Vector만을 정보로 받아 "i am a student"와 같은 결과를 출력합니다.

### Seq2Seq 개선
Seq2Seq 개선하는 방법 중 하나는 위에서 언급한 Attention Mechanism이 가장 인상 깊겠지만, 그전에 간단한 방법으로 개선하는 법을 언급하겠습니다. <br/>
=> 입력데이터의 순서를 뒤집는다. <br/>
**ex) "안녕 나는 딥러닝을 좋아하는 파이썬돌이야 ." => ". 파이썬돌이야 좋아하는 딥러닝을 나는 안녕"** <br/>
이렇게 언어 순서를 바꿔주는 것 만으로도 Context Vector에 다른 작용을해서 좋은 결과를 추출한다고 합니다.
###### 간단하게 원리를 위주로 알아봤으며, 파이토치 코드를 바탕으로 Pythonic하게 이해해보겠습니다. 해당 코드는 나동빈님의 깃허브를 바탕으로 제작되었음에 출처를 남깁니다. <br/>


## Pytorch Code
```python
import pytorch
import pytorch.nn as nn

class Encoder(nn.Module):
    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):
        super().__init__()

        # 임베딩(embedding)은 원-핫 인코딩(one-hot encoding)을 특정 차원의 임베딩으로 매핑하는 레이어
        self.embedding = nn.Embedding(input_dim, embed_dim)

        # LSTM 레이어
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)
        
        # 드롭아웃(dropout)
        self.dropout = nn.Dropout(dropout_ratio)

    # 인코더는 소스 문장을 입력으로 받아 문맥 벡터(context vector)를 반환        
    def forward(self, text):
        embedded = self.dropout(self.embedding(text))

        outputs, (hidden, cell) = self.rnn(embedded)
      
        # 문맥 벡터(context vector) 반환
        return hidden, cell
```  
```python
class Decoder(nn.Module):
    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):
        super().__init__()

        # 임베딩(embedding)은 원-핫 인코딩(one-hot encoding) 말고 특정 차원의 임베딩으로 매핑하는 레이어
        self.embedding = nn.Embedding(output_dim, embed_dim)

        # LSTM 레이어
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)
        
        # FC 레이어 (인코더와 구조적으로 다른 부분)
        self.output_dim = output_dim
        self.fc_out = nn.Linear(hidden_dim, output_dim)
        
        # 드롭아웃(dropout)
        self.dropout = nn.Dropout(dropout_ratio)

    # 디코더는 현재까지 출력된 문장에 대한 정보를 입력으로 받아 타겟 문장을 반환     
    def forward(self, input, hidden, cell):
        input = input.unsqueeze(0)
        
        embedded = self.dropout(self.embedding(input))
        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))

        prediction = self.fc_out(output.squeeze(0))
        
        return prediction, hidden, cell
```  
```python
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()

        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    # 학습할 때는 완전한 형태의 소스 문장, 타겟 문장, teacher_forcing_ratio를 넣기
    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        # 먼저 인코더를 거쳐 문맥 벡터(context vector)를 추출
        hidden, cell = self.encoder(src)

        # 디코더(decoder)의 최종 결과를 담을 텐서 객체 만들기
        trg_len = trg.shape[0] # 단어 개수
        batch_size = trg.shape[1] # 배치 크기
        trg_vocab_size = self.decoder.output_dim # 출력 차원
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)

        # 첫 번째 입력은 항상 <sos> 토큰
        input = trg[0, :]

        # 타겟 단어의 개수만큼 반복하여 디코더에 포워딩(forwarding)
        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell)

            outputs[t] = output # FC를 거쳐서 나온 현재의 출력 단어 정보
            top1 = output.argmax(1) # 가장 확률이 높은 단어의 인덱스 추출

            # teacher_forcing_ratio: 학습할 때 실제 목표 출력(ground-truth)을 사용하는 비율
            teacher_force = random.random() < teacher_forcing_ratio
            input = trg[t] if teacher_force else top1 # 현재의 출력 결과를 다음 입력에서 넣기
        
        return outputs
```  
<br/>

> 이미지/ 코드 출처
> * https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice ( 코드 )
> * http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ (이미지)
> * 밑바닥부터 시작하는 딥러닝 2 / 사이토 고키 / 한빛 미디어 
