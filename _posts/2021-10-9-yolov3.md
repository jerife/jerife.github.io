---
layout: post  
title: Yolo v3 이해하기
subtitle: 1-Stage Detection Model
gh-repo: jerife/jerife.github.io
gh-badge: [star, follow]
tags: [Computer Vision, Object Detection]
comments: true
---

# YOLO란?
<br/>
![yolo_img_1](https://user-images.githubusercontent.com/68190553/136654564-a8955779-5be8-4dd2-997d-a25a32d3160c.png){: .mx-auto.d-block :} <br/>

###### YOLO는 이전 포스팅인 2-stage 모델인 Faster RCNN과 다르게 대표적인 1-stage Object detection 모델입니다. <br/>
###### 1-stage와 2-stage의 차이를 알기위해 구조를 먼저 언급하겠습니다. <br/> <br/>
> ###### 1 stage :  이미지를 입력받아 한 모델에 한번에 Classification과 BoundingBox를 출력합니다. <br/>
> ###### 2 stage :  이미지를 입력받고 Feature map을 하여, RPN에서 Region Proposal을 받아 Feature map에 적용해 학습합니다.

![yolo_img_2](https://user-images.githubusercontent.com/68190553/136654678-fa54308a-3ff4-489c-8cee-ef4bd3fc2f30.png){: .mx-auto.d-block :} <br/>
###### 이는 Yolo v1의 구조입니다. Faster RCNN과 다르게 RPN부분이 따로 없는 것을 알 수 있습니다. 이 포스트에선 Yolo의 혁명이라 불리는 3번째(v3)버전 대해 다뤄보겠습니다.  <br/><br/>
###### Yolo v3의 구조는 크게 **"Backbone", "Neck", "Head"** 3가지로 나눌 수 있습니다. 순서대로 설명해보겠습니다. <br/> <br/>

## [1] Backbone
###### Backbone은 "Feature Extractor"라고도 불립니다. 이미지를 모델에 input으로 사용해, 이미지의 특징(feature map)을 ouptput으로 내보내는 모델입니다. <br/> <br/>
###### 대표적으론 CNN계열 모델들인 "ResNet", "DenseNet"등 Feature map을 출력하는 어떠한 모델들도 Backbone으로 사용될 수 있습니다. <br/><br/>
###### 하지만 Yolo v3에선 **DarkNet 53**이라는 모델을 사용했습니다. <br/>

![yolo_img_4](https://user-images.githubusercontent.com/68190553/136654864-4e9412ca-7cb6-447d-84e2-eb5ccf976036.png){: .mx-auto.d-block :}
> ###### Architecture of DarkNet 53<br/>

###### DarkNet 53의 구조는 위와 같습니다. Residual을 사용하는 것을 보아, ResNet의 Shot connection을 사용하는 것을 알 수 있습니다. <br/> <br/>
 
 
## [2] Neck
###### Neck은 Backbone에서 나온 Feature map을 더 **유의미하게** 만들어주는 작업을 해줍니다. 대표적으로 FPN(Feature Pyramid Network)라는 구조가 있습니다. <br/> <br/>

### [2-1] FPN
![yolo_img_3](https://user-images.githubusercontent.com/68190553/136657129-9be97224-d7bc-4dd3-8fc5-c9bbc4ed1296.png){: .mx-auto.d-block :} <br/>

###### Yolo의 Neck 구조를 설명하기 앞서 간단하게 FPN에 대해 정의하겠습니다. <br/> <br/>
###### 우리는 Backbone에서 여러단계를 거쳐 최종 Feature map을 출력합니다. 하지만 FPN은 중간 중간의 Feature map을 재사용해, 예측에 이용합니다. <br/> <br/>
#### **왜 이런 복잡한 과정을 거칠까요?** <br/>
###### Feature map은 층이 깊어질수록 사이즈가 작아지면서 추상적인 특징을 띄웁니다. <br/> <br/>
###### 하지만, 중간중간의 Feature map은 최종 Feature map보다 사실적인 특징(직각, 굴곡등 육안으로 확인하기 쉬운 특징)을 가지고 있기 때문에, 중간중간의 Feature map을 사용함으로써 더 유의미한 특징으로 모델을 이용할 수 있기때문입니다.<br/> <br/>

### [2-2] Yolo v3 Neck
 ![yolo_img_5](https://user-images.githubusercontent.com/68190553/136655327-217550b6-7364-4c75-bf42-31ec375ef4f5.PNG){: .mx-auto.d-block :}
> ###### Architecture of Yolo v3<br/>

###### 위 사진은 Yolo v3의 전체적인 구조를 보여줍니다. 사진의 좌측은 Backbone인 **DarkNet53**이며 Backbone의 최종, 중간, 초기 Feature map을 FPN에 사용합니다. <br/> <br/>
###### FPN에선 각각 크기가 다른 3개의 Feature map을 Upsampling 과정을 통해 크기를 맞춰주고 Concatnation해줍니다. <br/> <br/>
###### Neck부분에선 결과적으로 각각 크기가 다른 3개의 Feature map을 Head부분에 전달합니다. <br/> <br/>

## [3] Head
###### 마지막으로 Head부분입니다. Head는 이미지의 BoundingBox를 구하고 Classification해주는 부분입니다.<br/> <br/>

### [3-1] Anchor Box
###### Head부분에선 BoundingBox를 구하기 위해 Anchor Box라는 개념이 도입됍니다. Anchor Box의 내용은 [Faster R-CNN](https://jerife.github.io/2021-09-12-fasterrcnn/)포스트에 기재돼있으므로, 간단하게만 짚고 넘어가겠습니다.<br/> <br/>
![yolo_img_7](https://user-images.githubusercontent.com/68190553/136657152-bd423d75-79d9-4f26-a0b7-d0d886ca2c62.png){: .mx-auto.d-block :} <br/>
> ###### Pw, Ph: AnchorBox width, height
> ###### tx, ty, tw, th: Prediction Box
> ###### bx, by, bw, bh: 예측한 값을 학습할 수 있게 <br/>

###### yolov에서는 left top 꼭지점(Cx,Cy)으로부터 얼만큼 이동하는 지를 예측합니다. 이것이 bx=σ(tx) + cx가 의미하는 바입니다. 이때 σ(sigmoid)함수를 사용함으로써 0~1사이로 center좌표가 cell중심을 못벗어나게 조절해줍니다. <br/> <br/>
###### 다음으로 너비와 높이는 사전에 정의된 박스의 크기(Pw, Ph)를 얼만큼 비율로 조절할 지를 지수승을 통해 예측하며, bw=pwe^tw에 잘 나타나 있습니다. <br/> <br/>

### [3-2] Output Shape
###### 학습의 흐름을 한번 짚어보겠습니다. 
 - ###### (1) Backbone을 통해 3개의(최종, 중간, 초반) Feature map을 출력한다.
 - ###### (2) Neck에서 FPN을 통해 3개의 Feature map을 Upsampling과 Concatnation과정으로 3개 Feature map에 유의미한 특징을 부여한다.
 - ###### (3) 3개의 Feature map에 각각 Anchor Box기법을 적용해 BoudingBox를 구한후 Classification한다.
![yolo_img_6](https://user-images.githubusercontent.com/68190553/136657642-47369a3e-808c-4f59-b389-42ac541bb5d4.png){: .mx-auto.d-block :} <br/>
###### 이렇게 BoudingBox를 구하고 Classification까지 하면, 위와같은 Output이 나오게됩니다. <br/> <br/>
###### tx,ty,tw,th는 각 Boundbox의 좌표, Po는 Object일 확률, P1,P2,...,Pc는 클래스일 확률입니다. 만약 우리의 문제가 "강아지와 고양이분류" 문제라면, 클래스 확률은 P1(강아지),P2(고양이)로 2가지겠죠? <br/> <br/>
###### 특히 Yolo v3는 1Cell당 3가지 모양의 Anchor Box를 갖고 각 3가지의 스케일을 갖습니다. 즉, 1Cell당 9(3*3)개의 AnchorBox를 갖습니다.<br/> <br/>
###### 때문에 사진의 "B(Anchor Box의 갯수)"는 9일 것입니다. 여기까지가 Yolo v3의 구조입니다. 앞으로 우리는 이 정보를 바탕으로 Loss를 구하고 Backprobagation과정을 통해 점차 학습을 하게 될 것입니다.<br/> <br/>

## [4] Yolo Loss

![yolo_img_8](https://user-images.githubusercontent.com/68190553/136657673-881f68e5-1f16-4d5b-a0fb-4d6f4103cb98.png){: .mx-auto.d-block :} <br/>
###### Regression / Confidence / Classification 3개의 loss를 합해 구해집니다. 각 수식의 첫번쨰 시그마는 각 그리드마다의 loss를 구하기 위함이고, 두번째 시그마는 예측한 Bounding Box의 갯수를 의미합니다.<br/> <br/>
###### 1) Regression loss부터 확인해보겠습니다. 예측된 x,y좌표의 오차 제곱과, w,h의 루트를 취해 오차제곱한 값을 기반으로 구해집니다. <br/> <br/>
###### 2) 다음 Confidence loss는 예측된 Confidence Score와 Ground Truth의 IOU 오차를 기반으로 학습하며, 람다noob값을 이용해 물체가 없는 영역에 대한 값을 덜 반영합니다.<br/> <br/>
###### 3) 마지막으로 Classsiciation loss를 확인해보겠습니다. 예측 화률과 실제값의 오차의 제곱을 해준 값이며, Object를 책임지는 Bbox만 대상으로 학습을 합니다.<br/> <br/>

----
<br/>
![yolo_img_9](https://user-images.githubusercontent.com/68190553/136658281-05551dd0-734f-4c05-b69c-5a0e8727f4dd.jpg){: .mx-auto.d-block :} <br/>
###### 위 사진은 Yolo논문에 기재된 학습 결과입니다. 그래프를 넘어가면서까지 다른 모델들과 비교하는 것이 흥미롭네요. Yolo는 현재 v5까지 출시됐으며, 지금까지 가장 기본이 되는 v3모델을 정리함으로써 Yolo소개를 마무리하겠습니다.




